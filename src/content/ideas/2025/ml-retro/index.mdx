---
title: 'Machine Learning Retro'
description: 'A retrospective on my machine learning journey'
date: '2025-08-01'
status: 'in-progress'
type: 'project'
tags: ['machine-learning', 'python', 'supervised-learning', 'unsupervised-learning', 'reinforcement-learning', 'optimization']
category: ['projects']
draft: false
audience: 'All'
media_subpath: "/ideas/ml-retro/"
image:
  path: "./Paul_Signac_-_The_Port_of_Saint-Tropez.jpg"
  alt: 'The Port of Saint-Tropez by Paul Signac'
---

import BookCard from "../../../../components/BookCard.astro";
import Figure from "../../../../components/Figure.astro";

import mlMitchell from "../../../books/ml-mitchell.jpg";
import handsOnMl from "../../../books/hands-on-ml.jpg";
import dsDesignManual from "../../../books/ds-design-manual.jpg";
import introToStatLearning from "../../../books/intro-to-statistical-learning.jpg";
import aiModernApproach from "../../../books/ai-modern-approach-4th.jpg";
import rlSuttonBarto from "../../../books/rl-sutton-barto.jpg";
import grokkingDeepRL from "../../../books/grokking-deep-rl.jpg";

import mlTaxonomy from "./ml-taxonomy-placeholder.jpg";

## Introduction

This article is part personal reflection, part technical summary. I recently completed Georgia Tech's [CS 7641: Machine Learning](https://omscs.gatech.edu/cs-7641-machine-learning), and I wanted to take the time to solidify what I've learned—both for myself, in the spirit of the Feynman technique, and for others who might be curious about the course or about me. This includes future OMSCS students, hiring managers, and peers who want a better sense of what this course covers and what I now know (and don't know yet).

There's always a tension when writing something like this. I want to be honest about what I've learned and the depth of my understanding, while acknowledging that I'm still learning. This course was challenging and rewarding, and this write-up is my way of digesting the material and sharing it in a format that feels productive.

## Machine Learning

The course opens with a foundational discussion on the philosophy of machine learning. It's not simply "computational statistics," as one of the instructors, Dr. Michael Littman, argues. Instead, Dr. Charles Isbell proposes a broader definition:

> [!quote]
> "Machine learning is about this broader notion of building artifacts, computational artifacts typically, that learn over time based on experience. And then in particular, it's not just the act of building these artifacts that matter. It's the math behind it, it's the science behind it, it's the engineering behind it, and it's the computing behind it. It's everything that goes into building intelligent artifacts that almost by necessity have to learn over time." - Dr. Charles Isbell

This perspective—that machine learning is an interdisciplinary effort to create systems that genuinely learn—shapes the entire curriculum. The goal isn't just to use algorithms, but to understand them, to "think about data," and to "build artifacts that you know will learn." The course is structured around the three core pillars of machine learning: Supervised Learning, Unsupervised Learning, and Reinforcement Learning, which this article will also explore.

<Figure
  src={mlTaxonomy}
  alt="Machine Learning Taxonomy"
  caption="Machine Learning Taxonomy"
/>

## Supervised Learning

Supervised learning is a cornerstone of machine learning where the goal is to learn a mapping from inputs, or **instances**, to outputs. We are given a **sample** of training data, which consists of input vectors and their corresponding correct labels. The overall function that maps inputs to outputs is the **concept**, and the ideal version we are trying to discover is the **target concept**. The set of all possible concepts we might consider is the **hypothesis class**. Our learning algorithm will search through this hypothesis class to find a **candidate** concept that best approximates the target concept, which we then evaluate on a separate **testing set**.

### Classification and Regression

The two main types of supervised learning problems are classification and regression.
*   **Classification** aims to predict a discrete class label (e.g., "cat" or "dog"). The learned function is a decision boundary that separates the classes. We evaluate classification models with metrics like accuracy, precision, recall, and F1-score.
*   **Regression** aims to predict a continuous numerical value (e.g., stock price, temperature). The learned function is a curve or line that best fits the data points. We evaluate regression models with metrics like Mean Squared Error (MSE) or R-squared.

The fundamental difference lies in the output: classification models produce unordered, categorical outputs, while regression models produce ordered, continuous outputs. This distinction guides the choice of algorithms, loss functions, and evaluation metrics.

### Decision Trees

A Decision Tree is a non-parametric supervised learning method that approximates a target function with a tree-like model of decisions. It's an intuitive model, similar to a game of 20 questions, where each node in the tree represents a question about a feature, and the branches represent the answers, leading to a final decision at a leaf node.

The power of a decision tree lies in its expressiveness. It can represent simple logical functions like AND, OR, and XOR. However, the complexity of the tree can grow rapidly. For an n-ary OR function, the tree has a linear number of nodes, but for an n-ary XOR (parity), the number of nodes is exponential ($O(2^n)$). This highlights how the choice of features and the underlying structure of the problem dramatically impact model complexity.

The most common algorithm for building decision trees is **ID3**, which follows a simple loop:
1.  Select the "best" attribute to split the data on.
2.  Create a new node for that attribute.
3.  For each possible value of the attribute, create a new branch.
4.  Sort the training examples down to the new branches.
5.  If the examples are perfectly classified, stop. Otherwise, repeat the process on the new branches.

The "best" attribute is determined using a metric called **Information Gain**, which measures how much a given attribute reduces uncertainty, or **entropy**, in the data. {/** TODO: Add Information Gain formula back as an image or formatted text */}

ID3 has a **preference bias** towards shorter trees and trees that place good splits at the top. This is a form of Occam's Razor, preferring simpler explanations. However, this can lead to **overfitting**, where the tree becomes too specific to the training data. To combat this, a common technique is **pruning**, where parts of the tree are removed to improve its ability to generalize to new, unseen data.

Decision trees can also be adapted for regression problems by having the leaves predict a continuous value, often the average of the training samples that reach that leaf.


### Neural Networks

Artificial Neural Networks (ANNs) are a class of models inspired by the structure of the human brain. They are composed of layers of interconnected nodes, or "neurons." A typical network consists of an input layer, one or more hidden layers, and an output layer. Each connection between neurons has an associated weight, which is adjusted during the training process.

When data is fed into the network, each neuron receives inputs from the previous layer, calculates a weighted sum, and then passes the result through an activation function (like Sigmoid, Tanh, or ReLU) to produce its output. This process continues through the network until the final output is produced.

The network "learns" by adjusting its weights to minimize a loss function, which measures the difference between the network's predictions and the actual labels in the training data. The most common training algorithm is **backpropagation**, which uses gradient descent to efficiently compute the contribution of each weight to the overall error and adjust it accordingly.

### Ensemble Learning

Ensemble learning is a powerful technique that involves combining the predictions of multiple individual models to produce a more accurate and robust prediction than any single model. The core idea is that by aggregating the "votes" of a diverse set of models, we can reduce variance, mitigate bias, and improve generalization.

There are two primary families of ensemble methods:
*   **Bagging (Bootstrap Aggregating)**: This method involves training multiple instances of the same model (e.g., decision trees) on different random subsets of the training data (selected with replacement). The final prediction is made by averaging the outputs of all models (for regression) or by taking a majority vote (for classification). **Random Forests** are a popular and effective implementation of this approach.
*   **Boosting**: This method builds models sequentially, where each new model attempts to correct the errors made by the previous ones. The algorithm places more weight on the training instances that were misclassified by earlier models, forcing the new model to focus on the "hard" cases. Popular boosting algorithms include **AdaBoost** and **Gradient Boosting Machines (GBMs)**.

### Instance-based Learning

Instance-based learning, often called lazy learning, is a family of algorithms that, instead of building an explicit model from the training data, simply stores the entire dataset. When a new, unseen instance needs to be classified, the algorithm compares it to the stored instances and makes a prediction based on the most similar ones.

The most well-known instance-based learning algorithm is **k-Nearest Neighbors (k-NN)**. To classify a new data point, k-NN looks at the 'k' closest training examples in the feature space and assigns the new point the most common class among its neighbors. The "closeness" is typically measured using a distance metric like Euclidean distance.

The choice of 'k' is a critical hyperparameter. A small 'k' can make the model sensitive to noise, while a large 'k' can be computationally expensive and may oversmooth the decision boundary. Because they don't learn a "model" in the traditional sense, instance-based learners can easily adapt to new data but can also be very slow at prediction time, as they need to compare the new instance to every single training example.

### Kernel Methods & Support Vector Machines

Support Vector Machines (SVMs) are a powerful and versatile class of supervised learning models used for classification, regression, and outlier detection. The core idea of an SVM is to find the optimal hyperplane that best separates the classes in the feature space. The "best" hyperplane is the one that has the largest margin—the distance between the hyperplane and the nearest data points from either class. These nearest points are called the **support vectors**, as they are the critical elements that "support" the decision boundary.

One of the most powerful features of SVMs is the **kernel trick**. For data that is not linearly separable in its original feature space, SVMs can use a kernel function to implicitly map the data into a much higher-dimensional space where it *is* linearly separable. This is done without ever actually computing the coordinates of the data in this new space, which would be computationally very expensive.

Common kernel functions include:
*   **Linear**: For linearly separable data.
*   **Polynomial**: For data with polynomial relationships.
*   **Radial Basis Function (RBF)**: A very popular and flexible kernel that can handle complex, non-linear relationships.

By using the kernel trick, SVMs can create highly complex, non-linear decision boundaries, making them effective for a wide range of problems.

### Computational Learning Theory + VC Dimensions

Computational Learning Theory (COLT) is a field of study that seeks to answer fundamental questions about machine learning from a theoretical perspective. It asks: How can we be sure that a learning algorithm will generalize to new data? How many training examples are needed to guarantee good performance? How complex can our model be before it starts to overfit?

A key concept in COLT is the **Vapnik-Chervonenkis (VC) Dimension**. The VC dimension is a measure of the "capacity" or "expressive power" of a hypothesis class (the set of all possible models a learning algorithm can produce). Informally, it measures the most complex dataset that the model can "shatter"—that is, perfectly classify for every possible labeling of the data points.

A higher VC dimension means the model is more flexible and can fit more complex patterns. However, this comes at a cost. A model with a very high VC dimension requires more training data to learn effectively and is more prone to overfitting. The VC dimension provides a way to quantify the trade-off between the complexity of a model and its ability to generalize, giving us a theoretical handle on the bias-variance trade-off.

### Bayesian Learning + Inference

Bayesian learning is a probabilistic approach to machine learning that is based on **Bayes' Theorem**. Unlike frequentist methods that compute a single "best" model, Bayesian methods aim to calculate the full posterior probability distribution of the model's parameters, given the data.

{/* TODO: Add Bayes' Theorem formula back as an image or formatted text */}

The core idea is to start with a prior belief about our model's parameters, and then update this belief as we observe more data. This allows us to incorporate domain knowledge into our models and to quantify uncertainty in our predictions. For example, instead of predicting a single class, a Bayesian model can output a probability for each class, giving us a better sense of how confident the model is in its prediction.

## Unsupervised Learning
{/* TODO: Add content for Unsupervised Learning */}

### Randomized Optimization

Randomized optimization algorithms are a class of methods used to find good approximate solutions to complex optimization problems, especially those where the search space is too large or complex for traditional methods. They are often used in machine learning to tune the weights of a neural network, as an alternative to backpropagation.

*   **Randomized Hill Climbing (RHC)**: This is the simplest algorithm. It starts with a random solution and at each step, generates a random neighbor. If the neighbor is better, it moves to that neighbor. This process continues until no better neighbor can be found. The main drawback is that it can easily get stuck in local optima.
*   **Simulated Annealing (SA)**: SA improves on hill climbing by allowing the algorithm to occasionally accept worse solutions, which helps it escape local optima. The probability of accepting a worse solution is controlled by a "temperature" parameter, which starts high and gradually decreases over time. This mimics the metallurgical process of annealing, where a metal is heated and then cooled slowly to make it stronger.
*   **Genetic Algorithms (GAs)**: Inspired by the process of natural selection, GAs work with a population of solutions. At each iteration, the "fittest" solutions are selected to "reproduce" by combining their features (**crossover**) and introducing small random changes (**mutation**). This creates a new generation of solutions, and the process is repeated. GAs are effective at exploring large and complex search spaces.
*   **MIMIC (Mutual-Information-Maximizing Input Clustering)**: MIMIC is a more advanced algorithm that builds a probabilistic model of the promising regions of the search space. It samples new points from this model to generate the next generation of solutions. It can be very effective but is also more complex to implement and tune.

### Clustering

Clustering is a fundamental task in unsupervised learning where the goal is to group a set of objects in such a way that objects in the same group (or **cluster**) are more similar to each other than to those in other groups.

*   **k-Means Clustering**: This is the most popular clustering algorithm. It partitions the data into 'k' clusters by iteratively assigning each data point to the cluster with the nearest mean (or **centroid**) and then recalculating the centroids based on the new assignments. The algorithm converges when the assignments no longer change.
*   **Hierarchical Clustering**: This method creates a tree of clusters, known as a dendrogram. There are two main approaches: **agglomerative** (bottom-up), where each data point starts in its own cluster and pairs of clusters are merged as one moves up the hierarchy, and **divisive** (top-down), where all data points start in one cluster and splits are performed recursively as one moves down the hierarchy.
*   **Expectation-Maximization (EM)**: EM is a powerful algorithm often used for clustering with probabilistic models, like Gaussian Mixture Models (GMMs). It iteratively performs two steps: the **E-step (Expectation)**, where it calculates the probability that each data point belongs to each cluster, and the **M-step (Maximization)**, where it updates the parameters of the clusters to maximize the likelihood of the data.

### Feature Selection

Feature selection is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. The goal is to improve model performance by removing irrelevant or redundant features, which can reduce overfitting, improve accuracy, and reduce training time.

There are three main types of feature selection methods:
*   **Filter Methods**: These methods rank features based on some statistical score (like correlation or mutual information with the target variable) and then select the top-ranked features. They are fast and independent of the chosen learning algorithm.
*   **Wrapper Methods**: These methods use a specific machine learning algorithm to evaluate the quality of different subsets of features. They search for the combination of features that results in the best model performance, but they can be computationally expensive.
*   **Embedded Methods**: These methods perform feature selection as part of the model training process itself. A classic example is L1 regularization (Lasso), which adds a penalty to the model's loss function based on the absolute value of the feature weights, effectively shrinking the weights of less important features to zero.

### Feature Transformation

Feature transformation, or dimensionality reduction, is the process of transforming data from a high-dimensional space into a lower-dimensional space, while preserving some meaningful properties of the original data. This can help to visualize the data, reduce computational complexity, and remove noise.

*   **Principal Component Analysis (PCA)**: PCA is a linear technique that finds a new set of orthogonal axes, called principal components, that capture the maximum amount of variance in the data. The data is then projected onto a smaller number of these components.
*   **Independent Component Analysis (ICA)**: ICA is another linear technique that separates a multivariate signal into additive, non-Gaussian subcomponents. It is particularly useful for separating mixed signals, like separating individual voices from a mixed audio recording.
*   **Linear Discriminant Analysis (LDA)**: LDA is a supervised dimensionality reduction technique that finds the feature subspace that maximizes the separability between the classes. It is often used as a pre-processing step for classification tasks.
*   **t-SNE (t-Distributed Stochastic Neighbor Embedding)**: t-SNE is a non-linear technique that is particularly well-suited for visualizing high-dimensional datasets. It models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability.
*   **UMAP (Uniform Manifold Approximation and Projection)**: UMAP is another non-linear technique for manifold learning and dimensionality reduction. It is often faster than t-SNE and can better preserve the global structure of the data.


### Information Theory

Information theory is a mathematical framework for quantifying information. In the context of machine learning, it provides tools for measuring the amount of uncertainty and information in data.

The most fundamental concept is **entropy**, which measures the average level of "surprise" or "uncertainty" inherent in a variable's possible outcomes. In the context of decision trees, we use **information gain**, which is the reduction in entropy achieved by splitting the data on a particular feature. By always choosing the split that maximizes information gain, we can build a more efficient decision tree. Information theory also provides the foundation for other concepts like cross-entropy, which is a common loss function for classification problems, and mutual information, which can be used for feature selection.

## Reinforcement Learning
{/* TODO: Add content for Reinforcement Learning */}

### Markov Decision Processes

A Markov Decision Process (MDP) is a mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision-maker. It is the formal foundation for reinforcement learning.

An MDP is defined by:
*   A set of **states** (S)
*   A set of **actions** (A)
*   A **transition model** T(s, a, s'), which gives the probability of transitioning from state s to state s' after taking action a.
*   A **reward function** R(s, a, s'), which gives the immediate reward received after transitioning from state s to s' as a result of action a.

The goal in an MDP is to find a **policy** $\pi(s)$, which is a mapping from states to actions, that maximizes the cumulative expected reward over time.

### Reinforcement Learning

Reinforcement Learning (RL) is an area of machine learning concerned with how an **agent** ought to take **actions** in an **environment** in order to maximize some notion of cumulative **reward**. RL algorithms learn a policy by trial and error, interacting with the environment and receiving feedback in the form of rewards or punishments.

There are two main approaches to solving RL problems:
*   **Model-Based RL**: In this approach, the agent first tries to learn the transition model and reward function of the environment (i.e., learn the MDP). Once the model is learned, the agent can use planning algorithms (like value iteration or policy iteration) to find the optimal policy.
*   **Model-Free RL**: In this approach, the agent learns a policy directly without explicitly learning the environment's model. This is useful when the environment is too complex to model. Popular model-free algorithms include:
    *   **Q-Learning**: A value-based method that learns the optimal action-value function (Q-function), which represents the expected cumulative reward of taking a certain action in a certain state.
    *   **Policy Gradient Methods**: These methods learn the policy directly by adjusting its parameters in the direction that increases the expected reward.

### Game Theory

Game theory is the study of mathematical models of strategic interaction among rational decision-makers. In the context of machine learning, it is particularly relevant for multi-agent reinforcement learning, where multiple agents are interacting in the same environment.

Key concepts in game theory include:
*   **Players**: The decision-makers in the game.
*   **Strategies**: The possible actions that each player can take.
*   **Payoffs**: The outcomes or rewards that each player receives for a given combination of strategies.
*   **Nash Equilibrium**: A state of the game where no player has an incentive to unilaterally change their strategy, given the strategies of the other players.

By modeling multi-agent problems as games, we can analyze the dynamics of competition and cooperation and design agents that can learn to behave strategically in the presence of other intelligent agents.

## Course Overview

The projects were a great way to solidify the concepts learned in the course and took up a significant portion of the time I spent on the course.

Here are the projects from the course:
- **Project 1:** Supervised Learning
- **Project 2:** Randomized Optimization
- **Project 3:** Unsupervised Learning & Dimensionality Reduction
- **Project 4:** Reinforcement Learning

I'm planning to write deeper dives on each of the projects.

### Grading

Here's the grading breakdown from when I took the course in Spring 2024, based on the provided notes:

| Component | Weight |
|-----------|--------|
| Project 1 | 12.5%  |
| Project 2 | 12.5%  |
| Project 3 | 12.5%  |
| Project 4 | 12.5%  |
| Midterm Exam | 25%    |
| Final Exam | 25%    |


### Books

These are some classic books in Machine Learning that complement the course material.

{/* ML Mitchell, Hands-On ML, DS Design Manual, Intro to Stat Learning, AI Modern Approach, Grokking Deep RL */}

<BookCard
  title="Machine Learning"
  author="Tom Mitchell"
  img={mlMitchell}
>
  <p>
    A foundational text that provides a comprehensive introduction to machine learning. This is the core textbook for the course.
  </p>
</BookCard>

<BookCard
  title="Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow"
  author="Aurélien Géron"
  img={handsOnMl}
>
  <p>
    A hands-on guide to machine learning with Python. This is a great book for getting started with machine learning. This was particularly helpful for the first project.
  </p>
</BookCard>

<BookCard
  title="The Data Science Design Manual"
  author="Steve Skiena"
  img={dsDesignManual}
>
  <p>
    Provides a comprehensive overview of the data science process, from data collection to model deployment. I thought the treatment of data analysis and visualization was particularly helpful.
  </p>
</BookCard>

<BookCard
  title="An Introduction to Statistical Learning"
  author="Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani"
  img={introToStatLearning}
>
  <p>
    A great book for getting started with machine learning. It covers a lot of the same material as the course, but in a more statistical perspective. It's less rigorous than The Elements of Statistical Learning, but it's a great book for getting started.
  </p>
</BookCard>

<BookCard
  title="Artificial Intelligence: A Modern Approach"
  author="Stuart Russell and Peter Norvig"
  img={aiModernApproach}
>
  <p>
    A classic, often considered a bible for AI. It covers a vast range of topics from a philosophical perspective. The chapters on search, random optimization, unsupervised learning, and reinforcement learning were particularly helpful for the course.
  </p>
</BookCard>

<BookCard
  title="Reinforcement Learning: An Introduction"
  author="Richard S. Sutton and Andrew G. Barto"
  img={rlSuttonBarto}
>
    <p>
        The standard textbook for reinforcement learning, covering the core concepts and algorithms.
    </p>
</BookCard>

<BookCard
  title="Grokking Deep Reinforcement Learning"
  author="Miguel Morales"
  img={grokkingDeepRL}
>
  <p>
    A great book for getting started with reinforcement learning. It covers a lot of the same material as the course, but in a more mathematical perspective. This was particularly helpful for the fourth project.
  </p>
</BookCard>

### Other Resources

Here are some other posts I found helpful:
{/* TODO: Add links to other resources */}

---

## My Experience

{/* TODO: Add personal experience with the course */}

Thanks for reading!

## References
{/* TODO: Add references */}