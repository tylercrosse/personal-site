---
title: 'Machine Learning'
description: 'A survey of concepts covered in my graduate Machine Learning course'
date: '2025-08-01'
status: 'in-progress'
type: 'retro'
tags: ['machine-learning', 'python', 'supervised-learning', 'unsupervised-learning', 'reinforcement-learning', 'optimization']
category: ['mscs']
draft: false
audience: 'All'
media_subpath: "/ideas/ml-retro/"
image:
  path: "./Paul_Signac_-_The_Port_of_Saint-Tropez.jpg"
  alt: 'The Port of Saint-Tropez by Paul Signac'
---

import BookCard from "../../../../components/BookCard.astro";
import Figure from "../../../../components/Figure.astro";

import mlMitchell from "../../../books/ml-mitchell.jpg";
import handsOnMl from "../../../books/hands-on-ml.jpg";
import dsDesignManual from "../../../books/ds-design-manual.jpg";
import introToStatLearning from "../../../books/intro-to-statistical-learning.jpg";
import aiModernApproach from "../../../books/ai-modern-approach-4th.jpg";
import rlSuttonBarto from "../../../books/rl-sutton-barto.jpg";
import grokkingDeepRL from "../../../books/grokking-deep-rl.jpg";

import mlTaxonomy from "./ml-taxonomy2.png";
import classificationRegression from "./classification-regression-placeholder.png";
import decisionTree from "./decision-tree-placeholder.png";
import inductiveBias from "./inductive-bias-placeholder.webp";
import ann from "./ann-placeholder.webp";
import nnWeights from "./nn-weights-placeholder.jpg";
import xorSeparability from "./xor-separability-placeholder.png";
import gradDescent from "./grad-descent-placeholder.jpg";
import baggingBoosting from "./bagging-boosting-placeholder.jpg";
import instanceBased from "./instance-based-placeholder.png";
import kernelTrick from "./kernel-trick-placeholder.png";
import pac from "./pac-placeholder2.png";
import activationFunctions from "./activation-funcs-placeholder.jpg";
import aiMlDl from "./ai-ml-dl-placeholder.webp";
import svmMargin from "./svm-margin-placeholder.png";
import vcDimension from "./vc-dimension-placeholder.png";
import bayesianInference from "./bayesian-inference-placeholder.jpg";
import simulatedAnnealing from "./simulated-annealing.gif";
import geneticAlg from "./genetic-alg-placeholder.svg";
import kMeansEmClustering from "./k-means-em-clustering-placeholder.png";
import pcaIca from "./pca-ica-placeholder.png";
import tsneUmap from "./tsne-umap-placeholder.png";
import mdp from "./mdp-placeholder.png";
import rlModels from "./rl-models-placeholder.png";
import nash from "./nash-placeholder.png";

## Introduction

This article is part personal reflection, part technical summary. I recently completed Georgia Tech's [CS 7641: Machine Learning](https://omscs.gatech.edu/cs-7641-machine-learning), and I wanted to take the time to solidify what I've learned—both for myself, in the spirit of the Feynman technique, and for others who might be curious about the course or about me. This includes future OMSCS students, hiring managers, and peers who want a better sense of what this course covers and what I now know (and don't know yet). The majority of this article is a summary of the course. I've also included a section on the mechanics of the course, my experience and resources at the end.

There's always a tension when writing something like this. I want to be honest about what I've learned and the depth of my understanding, while acknowledging that I'm still learning. This course was challenging and rewarding, and this write-up is my way of digesting the material and sharing it in a format that feels productive.

## Machine Learning

Machine learning is often framed as a subfield of artificial intelligence (AI), but it's more than that. It's a broad field that encompasses a wide range of techniques and applications. It's a field that is constantly evolving, and there is no single "right" way to do machine learning. Recent advances in machine learning have been driven by the development of deep learning, which at least started as a subfield of machine learning that uses deep neural networks to learn from data.

<Figure
  src={aiMlDl}
  alt="Machine Learning, AI, and Deep Learning"
  caption="Machine Learning, AI, and Deep Learning"
/>

The course is taught as a dialog between Dr. Michael Littman and Dr. Charles Isbell. It opens with a foundational discussion on the philosophy of machine learning. It's not simply "computational statistics," as Dr. Littman argues. Instead, Dr. Isbell proposes a broader definition:

> [!quote]
> "Machine learning is about this broader notion of building artifacts, computational artifacts typically, that learn over time based on experience. And then in particular, it's not just the act of building these artifacts that matter. It's the math behind it, it's the science behind it, it's the engineering behind it, and it's the computing behind it. It's everything that goes into building intelligent artifacts that almost by necessity have to learn over time." - Dr. Charles Isbell

This perspective—that machine learning is an interdisciplinary effort to create systems that genuinely learn—shapes the entire curriculum. The goal isn't just to use algorithms, but to understand them, to "think about data," and to "build artifacts that you know will learn." 

Three core pillars of machine learning make up the foundation of the course: supervised learning, unsupervised learning, and reinforcement learning. Each of these pillars is a different way of learning from data. The most common and well-understood is **Supervised learning** which involves learning from labeled data. **Unsupervised learning** is a way to learn from data without labels, and **Reinforcement learning** is a way to learn from data by interacting with an environment.

<Figure
  src={mlTaxonomy}
  alt="Machine Learning Taxonomy"
  caption="Machine Learning Taxonomy"
/>

Math is the language of machine learning. Feel free to gloss over it if it's off putting, there's still value in building intuition. Defining a few key terms helps to enrich the discussion:

- **Data point ($x$)**: The individual instances of data we want to make predictions about. An instance is typically represented as a vector of features or attributes. The data points are typically a finite sample drawn i.i.d. from some unknown distribution $P(x)$ over the inputs space $\mathcal{X}$  of all possible values of $x$.
- **Labels ($y$)**: The correct answers we want to predict. A label is typically a single value, like a color ("red," "blue," "green") or a simple "yes" or "no." The output space $\mathcal{Y}$ is the set of all possible values of $y$ and $P(y)$ is the distribution of the labels. The big distinction between supervised and unsupervised learning is whether the labels are provided or not.
- **Sample (or Training Set) ($\mathcal{D}$)**: This is the data we learn from. It's a set of instances that the learning algorithm uses to find a function within the hypothesis class that best fits the data.
- **Concept ($f$)**: A concept is the function we are trying to learn. It's a mapping from input instances to outputs. Intuitively, a concept is the underlying idea we want our model to grasp. 
  - In **supervised learning** the classic formulation is $f: \mathcal{X} \rightarrow \mathcal{Y}$, the idea of learning a 'concept' from a 'hypothesis class'.
  - In **unsupervised learning**, the concept is not a mapping to pre-defined labels, but a function that reveals underlying structure, like a clustering assignment ($f: \mathcal{X} \rightarrow \{c_1, ..., c_k\}$) or a lower-dimensional projection.
  - In **reinforcement learning**, the core concept being learned is a **policy** ($\pi$), which is a function mapping states to actions ($\pi: S \rightarrow A$) to maximize long-term reward.
*   **Target Concept ($f^*$)**: This is the *true*, ideal function that we aim to approximate. It's the perfect, ground-truth mapping that we will never truly know but strive to find.


With these terms in mind, the course is structured around the three core pillars of machine learning: Supervised Learning, Unsupervised Learning, and Reinforcement Learning, which this article will also explore.

## Supervised Learning

Supervised learning is a cornerstone of machine learning. The goal is to learn from labeled data. To understand how this works, it's helpful to define a few key terms that form the language of supervised learning:

{/* something about y, and the data in supervised learning be {x, y} etc. */}

*   **Hypothesis Class ($H$)**: This is the set of all possible concepts our learning algorithm is allowed to consider. By choosing a type of model (like decision trees or neural networks), we are implicitly defining our hypothesis class.
*   **Candidate Concept**: This is a concept from the hypothesis class that our algorithm believes might be the target concept. The learning process involves evaluating different candidate concepts.
*   **Testing Set**: This is a separate set of labeled instances, unseen during training, that we use to evaluate our final candidate concept. A model's performance on the testing set indicates its ability to **generalize** to new, unseen data, which is the ultimate goal of machine learning. The training and testing sets must be distinct to avoid "cheating."

With these terms in mind, the process is to use a learning algorithm to search through the hypothesis class ($H$) using the training set to find a final candidate concept that is a good approximation of the target concept ($f^*$), as measured by its performance on the testing set.

The supervised learning hypothesis classes we will explore are:

*   **Decision Trees**: A tree-like model of decisions.
*   **Neural Networks**: A model that is inspired by the structure of the human brain.
*   **Ensemble Methods**: A model that is a combination of multiple models.
*   **Instance-based Learning**: A model that is a combination of multiple models.
*   **Kernel Methods & Support Vector Machines**: A model that is a combination of multiple models.


### Classification and Regression

The two main types of concepts $f$ that can be learned with supervised learning are classification and regression. The key distinction is the nature of the output.

*   **Classification** aims to predict a discrete class label. The output is a category, like a color ("red," "blue," "green") or a simple "yes" or "no." The learned function is a decision boundary that separates the classes. For example, a bank might use a classification model to decide whether to lend money based on a person's credit history. The output is a discrete choice: lend or not. We evaluate classification models with metrics like accuracy, precision, recall, and F1-score.
*   **Regression** aims to predict a continuous numerical value. The output is a real number, like a stock price, temperature, or a person's age. The learned function is a curve or line that best fits the data points. We evaluate regression models with metrics like Mean Squared Error (MSE) or R-squared.

<Figure
  src={classificationRegression}
  alt="Classification and Regression"
  caption="Classification and Regression"
/>

The fundamental difference lies in the output: classification models produce unordered, categorical outputs, while regression models produce ordered, continuous outputs. This distinction guides the choice of algorithms, loss functions, and evaluation metrics. A problem like predicting a person's age could be framed as either. If you predict their exact age (e.g., 23.7 years), it's regression. If you predict an age group (e.g., "high school," "college," "grad student"), it's a classification task.

### Decision Trees

A Decision Tree is a non-parametric supervised learning method that approximates a target function with a tree-like model of decisions. It's an intuitive model, much like a game of **20 Questions**. Each internal node in the tree represents a question about a feature, and the branches represent the answers, leading you down a path to a final decision at a leaf node.

The most common algorithm for building decision trees is **ID3**, which follows a simple, greedy, top-down approach. It's best understood through the "20 Questions" analogy: at each step, you want to ask the question that gives you the most information, splitting the remaining possibilities as cleanly as possible.

The ID3 loop is as follows:
1.  Select the "best" attribute to split the data on.
2.  Create a new decision node for that attribute.
3.  For each possible value of the attribute, create a new branch.
4.  Sort the training examples down to the new branches.
5.  If a branch contains perfectly classified examples, you're done for that path and can add a leaf node. Otherwise, you repeat the process on the remaining examples in that branch.

<Figure
  src={decisionTree}
  alt="Decision Tree"
  caption="Decision Tree"
/>

The "best" attribute is the one that provides the highest **Information Gain**. This metric measures how much a given attribute reduces uncertainty, or **entropy** ($H$), in the data. Entropy is a measure of randomness or impurity. A dataset with an even split of class labels (e.g., 50% "yes," 50% "no") has high entropy, while a dataset with only one class label (e.g., 100% "yes") has zero entropy. Information gain selects the attribute that leads to the "purest" child nodes—the splits that do the best job of separating the data by their class labels. More formally, the entropy of an attribute $X$ is defined as:

$$
H(X) = -\sum_{i=1}^{n} p_i \log_2(p_i)
$$

- $H(X)$ is the entropy of the attribute $X$.
- $n$ is the number of classes in the attribute $X$.
- $p_i$ is the probability of the $i$th class in the attribute $X$.
- $\log_2$ is the base-2 logarithm, reflecting the bits of information needed to specify the outcome. The base-2 is used because we split between two choices (e.g. binary, yes/no) at each node.

Altogether, the entropy of an attribute $X$ is the sum of the probability of each class $i$ in $X$ multiplied by the logarithm of that probability. ID3 aims to maximize the reduction in this entropy at each step.

{/* Maybe omit this paragraph? I'm not sure it adds much to the discussion. */}
{/* The power of a decision tree lies in its expressiveness. It can represent any Boolean function. For simple logical functions like AND and OR, the tree is compact. For example, an OR function with inputs A and B can be represented with just a few nodes: if A is true, output true; otherwise, check B. However, the complexity can grow rapidly. For the "any of" function (an n-ary OR function), the tree has a linear number of nodes ($O(n)$) -- one for each option. But for the "if and only if" function a.k.a "parity" (an n-ary XOR function), the number of nodes is exponential ($O(2^n)$). For example, for the inputs A and B: you need to check both A and B at every level, creating branches for all combinations (A=true,B=false → true; A=false,B=true → true; otherwise false). This highlights a key challenge: some concepts are inherently "harder" for decision trees to learn and represent efficiently. If your problem has a parity-like structure, you might need a very deep tree or a different representation altogether. */}

#### 20 Questions Example

When playing 20 Questions, you want to ask the question that gives you the most information. For example, if you are trying to guess a person's name, you might ask if the first letter comes before or after "M". This is a binary question, and it gives you the most information because it splits the remaining possibilities in half.

This example also illustrates how decision trees can handle continuous attributes. The algorithm might ask a binary question like "Is age < 25.5?". The optimal split point is typically found by sorting all the values of the continuous attribute in the training data and testing the midpoint between each consecutive pair. This allows you to repeat an attribute along a path (e.g., first ask "Is age < 25.5?" then later "Is age < 20?") because each split represents a new, distinct question.

#### Inductive Bias and Overfitting in Decision Trees

Every type of model (hypothesis class) has a different **inductive bias**. This is the set of assumptions the model makes about the data. Inductive bias can be thought of as the model's "prior" knowledge about the data. It comes from the model's design and structure.

**Preference bias** is the model's tendency to favor certain types of solutions (hypotheses) over others within the chosen hypothesis space. **Restriction bias** is the model's tendency to favor solutions that are consistent with the data, limiting the set of possible hypotheses that the algorithm can even consider. Both are forms of inductive bias.

<Figure
  src={inductiveBias}
  alt="Inductive Bias"
  caption="Inductive Bias"
/>

For example, decision trees assume that the data is split into a tree-like structure. This is a form of restriction bias. The greedy nature of ID3 gives it a specific preference bias for:
1.  **Good splits at the top:** It will always choose the split that maximizes information gain at the current node.
2.  **Shorter trees:** As a consequence of the first point, it naturally prefers simpler explanations that classify the data quickly. This is a form of Occam's Razor.

However, this greedy approach can lead to **overfitting**. The tree might grow very large to perfectly classify every training example, including noisy data. Such a tree would fail to generalize to new, unseen data. To combat this, a common technique is **pruning**, where parts of the tree are removed post-training to improve its ability to generalize. Another approach is to set a stopping criterion, such as a maximum tree depth or a minimum number of samples required at a leaf node.

### Neural Networks

Artificial Neural Networks (ANNs) are models loosely inspired by the structure of the human brain, but as the course amusingly puts it, they are more of a "cartoonish version of the neuron." The fundamental building block is the **perceptron**, a simple computational unit that takes several inputs, multiplies them by weights, sums them up, and fires (outputs a 1) if the sum exceeds a certain threshold, otherwise it outputs a 0.

<Figure
  src={ann}
  alt="Artificial Neural Network"
  caption="A single perceptron on the left, which can't solve XOR. A network of perceptrons on the right, which can."
/>

Modern neural networks build on this idea but replace the simple, sharp threshold of the perceptron with a smooth, differentiable **activation function** like the Sigmoid, Tanh, or ReLU.

The shift from the simple threshold values to using activation functions took is uderpinned by the idea of **linear separability**. A single perceptron can act as a linear separator; for a 2D input space, it can draw a single straight line to divide the space into two halves. This is powerful enough to represent simple logical functions like AND, OR, and NOT, which are all linearly separable. However, a single perceptron fundamentally cannot solve problems that are *not* linearly separable. The classic example is the XOR function—there is no single straight line you can draw to separate the true cases from the false cases.

<Figure
  src={xorSeparability}
  alt="XOR Separability"
  caption="XOR is not linearly separable."
/>


This limitation is what gives rise to the "network" in neural networks. To solve a problem like XOR, you need to combine perceptrons. For instance, you could use one layer of perceptrons to learn simpler functions (like AND and OR) and then feed their outputs into a final perceptron that combines them to produce the final, non-linear result. This layered structure allows ANNs to learn increasingly complex functions.


<Figure
  src={nnWeights}
  alt="Neural Network Weights"
  caption="Weights in a neural network are adjusted during training to minimize error."
/>

Why the switch? The "pointy" nature of the threshold function isn't differentiable, which is a problem for training. A smooth function allows us to use calculus. The network "learns" by adjusting its weights to minimize a loss function, which measures the difference between the network's predictions and the actual labels in the training data. The most common training algorithm is **backpropagation**. At its heart, backpropagation is a clever application of the chain rule from calculus. It uses **gradient descent** to efficiently compute the contribution of each weight to the overall error and adjust it accordingly. This process is often visualized as error information "flowing" backward from the output layer through the hidden layers, hence the name.

<Figure
  src={gradDescent}
  alt="Gradient Descent"
  caption="Gradient Descent"
/>


This training process also reveals the biases of neural networks.
*   **Restriction Bias**: How much does the model's structure limit the functions it can represent? For ANNs, the restriction bias is very low. With enough layers and nodes, a neural network can approximate *any* continuous function (and with two hidden layers, any arbitrary function). This makes them universal approximators but also puts them at high risk of **overfitting**—modeling the noise in the training data instead of the underlying pattern.
*   **Preference Bias**: What kinds of solutions does the learning algorithm prefer? The algorithm itself provides a bias. Training typically starts with small, random weights. As the network trains via gradient descent, it gradually explores more complex functions. This means the algorithm has a preference for simpler explanations, only adopting more complexity as the data requires it. This is a form of Occam's Razor, and techniques like **early stopping** (using a validation set to stop training before the model overfits) explicitly leverage this property.

The power of neural networks lies in this combination of a highly expressive representation (low restriction bias) with a training process that prefers simpler solutions (a helpful preference bias).

<details>
  <summary>For more details, see the <a href="https://www.youtube.com/watch?v=1-4c0_gLSSg&list=PLTsf9UeqkReZbK7xqIYn_mXmsQZIb011T&index=2" target="_blank">Neural Networks lecture</a> which covers:</summary>
  - How to manually set weights and thresholds for a single perceptron to compute logical functions like AND, OR, and NOT.
  - The step-by-step construction of a two-layer network that can solve the non-linearly separable XOR problem.
  - The Perceptron Training Rule for single units and its guarantee of convergence for linearly separable data.
  - The distinction between the Perceptron Rule (using thresholded outputs) and the Delta Rule / Gradient Descent (using unthresholded activation).
  - The common trick of using a "bias input" to allow the learning algorithm to learn the threshold as just another weight.
  - The problem of getting stuck in local optima during training and advanced optimization techniques to mitigate it, such as momentum.
</details>


### Instance-based Learning

Instance-based learning flips the script on the typical machine learning paradigm. Most models we've discussed so far are **eager learners**: they take the training data, learn a generalized function or model, and then effectively throw the original data away. When a new query comes in, they use this compact model to make a prediction. Instance-based learners, in contrast, are **lazy learners**. They do almost no work upfront, simply storing the entire training dataset. The real computation happens at query time.

This approach is best understood through its most famous algorithm: **k-Nearest Neighbors (k-NN)**. Imagine you're trying to predict a house price. Instead of building a complex regression model for the whole city, you just find the 'k' most similar houses (the "neighbors") to the one you're interested in and see what they sold for.

<Figure
  src={instanceBased}
  alt="Instance-based Learning"
  caption="k-Nearest Neighbors finds the 'k' closest points in the training data to make a prediction."
/>

The algorithm is beautifully simple:
1.  **Store** all the training data.
2.  When you get a new query point, **calculate the distance** from that point to every single point in the training data.
3.  **Identify the 'k' nearest neighbors**.
4.  **Aggregate their outputs**. For classification, this means taking a majority vote among the neighbors' classes. For regression, it means averaging their values.

This simplicity is deceptive, as the real "learning" is encoded in two key design choices that represent crucial domain knowledge:
*   **The distance metric**: How do you define "similarity"? For points on a map, Euclidean distance might work. But what about other features? Are all features equally important? A good distance function is critical and is a way to inject domain knowledge into the model. Using a standard metric like Euclidean or Manhattan distance implicitly assumes all features matter equally, which is rarely true.
*   **The value of k**: How many neighbors should you consider? A small 'k' makes the model sensitive to noise (an outlier could become a nearest neighbor). A large 'k' smooths things out but can be computationally expensive and may blur the lines between distinct regions in your data.

The biggest challenge for k-NN, and a foundational problem in machine learning, is the **Curse of Dimensionality**. As you add more features (dimensions) to your data, the volume of the space grows exponentially. To maintain the same density of data points, you need an exponentially larger dataset. In a high-dimensional space, the concept of a "neighborhood" breaks down—every point is far away from every other point. Irrelevant features can easily drown out the signal from the important ones, making the distance metric less meaningful.

<details>
  <summary>For more details, see the <a href="https://www.youtube.com/watch?v=k_t3S8y3V-A&list=PLTsf9UeqkReZbK7xqIYn_mXmsQZIb011T&index=3" target="_blank">Instance-Based Learning lecture</a> which covers:</summary>
  - A comparison of time and space complexity for "lazy" learners (like k-NN) vs. "eager" learners (like linear regression).
  - The idea of using distance-weighted averaging or voting to give closer neighbors more influence.
  - An example of how different distance metrics (Euclidean vs. Manhattan) can produce wildly different results for the same query.
  - How k-NN's bias towards locality and smoothness means it can perform poorly when features have different levels of importance.
  - The concept of **locally weighted regression**, where instead of just averaging neighbors, you fit a model (like a line) to them, allowing a simple local model to represent a complex global function.
</details>


### Ensemble Learning

Ensemble learning is a powerful technique that works by combining multiple individual models—often simple "rules of thumb"—to produce a more accurate and robust prediction than any single model could. The lecture uses the great example of classifying spam email. You could have a simple rule like "the email contains the word 'manly'", and another like "the email is very short". Neither is very accurate on its own, but by combining a large set of these weak indicators, you can build a highly effective classifier.

The power of this approach is that the final combined hypothesis can be much more complex and expressive than any of the individual weak learners. By taking a weighted combination of many simple models (like shallow decision trees), the ensemble can form a sophisticated, non-linear decision boundary that it couldn't have represented otherwise. It's a way of getting a complex result from simple parts.

The two primary families of ensemble methods, Bagging and Boosting, are different approaches to this same core idea.

<Figure
  src={baggingBoosting}
  alt="Bagging and Boosting"
  caption="Bagging trains models in parallel on random subsets. Boosting trains models sequentially, focusing on hard examples."
/>

*   **Bagging (Bootstrap Aggregating)**: This method follows a simple and surprisingly effective process. It involves training multiple instances of the same model (e.g., decision trees) on different random subsets of the training data. The subsets are created by **bootstrapping**, which means sampling with replacement. The final prediction is made by averaging the outputs of all models (for regression) or by taking a majority vote (for classification). **Random Forests** are a popular and effective implementation of this approach. The main benefit of bagging is that it reduces variance and helps prevent overfitting. By averaging out the "opinions" of models trained on different views of the data, it's less likely to be swayed by noise.

*   **Boosting**: This method takes a more strategic approach. Instead of training on random subsets, boosting builds models sequentially, where each new model is explicitly tasked with correcting the errors made by the previous ones. The algorithm maintains a distribution of "weights" over the training examples. In each round, it trains a **weak learner**—an algorithm that only needs to be slightly better than random chance—on the weighted data. After each round, it increases the weights of the examples that were misclassified, forcing the next weak learner to focus on the "hard" cases. The final model is a weighted combination of all the weak learners, where models that performed better on their respective rounds are given a greater say in the final prediction. Popular boosting algorithms include **AdaBoost** and **Gradient Boosting Machines (GBMs)**.

Perhaps the most remarkable property of boosting is its surprising resistance to overfitting. With most algorithms, as we continue to train, the model gets better and better on the training data, but at some point, the performance on unseen test data starts to get worse—the classic sign of overfitting. Boosting often defies this. In practice, even as the training error goes to zero, the test error can continue to improve as more weak learners are added. While the full explanation is complex, the intuition is that even after the model correctly classifies all training points, adding more weak learners can increase the *margin* of the classification, making the decision boundary more robust and improving generalization.

<details>
  <summary>For more details, see the <a href="https://www.youtube.com/watch?v=tAio4_2o5k8&list=PLTsf9UeqkReZbK7xqIYn_mXmsQZIb011T&index=4" target="_blank">Ensemble Learning lecture</a> which covers:</summary>
  - The formal definition of a "weak learner" as a classifier that can always perform slightly better than chance (error < 0.5) on any distribution of data.
  - A visual walkthrough of how boosting combines simple axis-aligned classifiers to create a complex, non-linear decision boundary.
  - The specific formulas used in the AdaBoost algorithm to update the data distribution and calculate the `alpha` weights for each weak learner.
</details>

### Kernel Methods & Support Vector Machines

Support Vector Machines (SVMs) are a powerful and versatile class of supervised learning models used for classification, regression, and outlier detection. The core idea of an SVM is to find the optimal hyperplane that best separates the classes in the feature space. The "best" hyperplane is the one that has the largest margin—the distance between the hyperplane and the nearest data points from either class. These nearest points are called the **support vectors**, as they are the critical elements that "support" the decision boundary.

<Figure
  src={svmMargin}
  alt="Support Vector Machine Margin"
  caption="Support Vector Machine Margin"
/>

One of the most powerful features of SVMs is the **kernel trick**. For data that is not linearly separable in its original feature space, SVMs can use a kernel function to implicitly map the data into a much higher-dimensional space where it *is* linearly separable. This is done without ever actually computing the coordinates of the data in this new space, which would be computationally very expensive.

<Figure
  src={kernelTrick}
  alt="Kernel Trick"
  caption="Kernel Trick"
/>

Common kernel functions include:
*   **Linear**: For linearly separable data.
*   **Polynomial**: For data with polynomial relationships.
*   **Radial Basis Function (RBF)**: A very popular and flexible kernel that can handle complex, non-linear relationships.

By using the kernel trick, SVMs can create highly complex, non-linear decision boundaries, making them effective for a wide range of problems.

### Computational Learning Theory + VC Dimensions

Computational Learning Theory (COLT) is a field of study that seeks to answer fundamental questions about machine learning from a theoretical perspective. It asks: How can we be sure that a learning algorithm will generalize to new data? How many training examples are needed to guarantee good performance? How complex can our model be before it starts to overfit?

A key concept in COLT is the **Vapnik-Chervonenkis (VC) Dimension**. The VC dimension is a measure of the "capacity" or "expressive power" of a hypothesis class (the set of all possible models a learning algorithm can produce). Informally, it measures the most complex dataset that the model can "shatter"—that is, perfectly classify for every possible labeling of the data points.

<Figure
  src={vcDimension}
  alt="VC Dimension"
  caption="VC Dimension"
/>

A higher VC dimension means the model is more flexible and can fit more complex patterns. However, this comes at a cost. A model with a very high VC dimension requires more training data to learn effectively and is more prone to overfitting. The VC dimension provides a way to quantify the trade-off between the complexity of a model and its ability to generalize, giving us a theoretical handle on the bias-variance trade-off.

<Figure
  src={pac}
  alt="PAC Learning"
  caption="PAC Learning"
/>

### Bayesian Learning + Inference

Bayesian learning is a probabilistic approach to machine learning that is based on **Bayes' Theorem**. Unlike frequentist methods that compute a single "best" model, Bayesian methods aim to calculate the full posterior probability distribution of the model's parameters, given the data.

$$
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
$$

- $P(A|B)$ is the posterior probability of $A$ given $B$.
- $P(B|A)$ is the likelihood of $B$ given $A$.
- $P(A)$ is the prior probability of $A$.
- $P(B)$ is the prior probability of $B$.


The core idea is to start with a prior belief about our model's parameters, and then update this belief as we observe more data. This allows us to incorporate domain knowledge into our models and to quantify uncertainty in our predictions. For example, instead of predicting a single class, a Bayesian model can output a probability for each class, giving us a better sense of how confident the model is in its prediction.

<Figure
  src={bayesianInference}
  alt="Bayesian Inference"
  caption="Bayesian Inference"
/>

## Unsupervised Learning
{/* TODO: Add overview of unsupervised learning */}

### Randomized Optimization

Randomized optimization algorithms are a class of methods used to find good approximate solutions to complex optimization problems, especially those where the search space is too large or complex for traditional methods. They are often used in machine learning to tune the weights of a neural network, as an alternative to backpropagation.

*   **Randomized Hill Climbing (RHC)**: This is the simplest algorithm. It starts with a random solution and at each step, generates a random neighbor. If the neighbor is better, it moves to that neighbor. This process continues until no better neighbor can be found. The main drawback is that it can easily get stuck in local optima.
*   **Simulated Annealing (SA)**: SA improves on hill climbing by allowing the algorithm to occasionally accept worse solutions, which helps it escape local optima. The probability of accepting a worse solution is controlled by a "temperature" parameter, which starts high and gradually decreases over time. This mimics the metallurgical process of annealing, where a metal is heated and then cooled slowly to make it stronger.

<Figure
  src={simulatedAnnealing}
  alt="Simulated Annealing"
  caption="Simulated Annealing"
/>

*   **Genetic Algorithms (GAs)**: Inspired by the process of natural selection, GAs work with a population of solutions. At each iteration, the "fittest" solutions are selected to "reproduce" by combining their features (**crossover**) and introducing small random changes (**mutation**). This creates a new generation of solutions, and the process is repeated. GAs are effective at exploring large and complex search spaces.

<Figure
  src={geneticAlg}
  alt="Genetic Algorithm"
  caption="Genetic Algorithm"
/>

*   **MIMIC (Mutual-Information-Maximizing Input Clustering)**: MIMIC is a more advanced algorithm that builds a probabilistic model of the promising regions of the search space. It samples new points from this model to generate the next generation of solutions. It can be very effective but is also more complex to implement and tune.

### Clustering

Clustering is a fundamental task in unsupervised learning where the goal is to group a set of objects in such a way that objects in the same group (or **cluster**) are more similar to each other than to those in other groups.

<Figure
  src={kMeansEmClustering}
  alt="k-Means and EM Clustering"
  caption="k-Means and EM Clustering"
/>

*   **k-Means Clustering**: This is the most popular clustering algorithm. It partitions the data into 'k' clusters by iteratively assigning each data point to the cluster with the nearest mean (or **centroid**) and then recalculating the centroids based on the new assignments. The algorithm converges when the assignments no longer change.
*   **Hierarchical Clustering**: This method creates a tree of clusters, known as a dendrogram. There are two main approaches: **agglomerative** (bottom-up), where each data point starts in its own cluster and pairs of clusters are merged as one moves up the hierarchy, and **divisive** (top-down), where all data points start in one cluster and splits are performed recursively as one moves down the hierarchy.
*   **Expectation-Maximization (EM)**: EM is a powerful algorithm often used for clustering with probabilistic models, like Gaussian Mixture Models (GMMs). It iteratively performs two steps: the **E-step (Expectation)**, where it calculates the probability that each data point belongs to each cluster, and the **M-step (Maximization)**, where it updates the parameters of the clusters to maximize the likelihood of the data.

### Feature Selection

Feature selection is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. The goal is to improve model performance by removing irrelevant or redundant features, which can reduce overfitting, improve accuracy, and reduce training time.

There are three main types of feature selection methods:
*   **Filter Methods**: These methods rank features based on some statistical score (like correlation or mutual information with the target variable) and then select the top-ranked features. They are fast and independent of the chosen learning algorithm.
*   **Wrapper Methods**: These methods use a specific machine learning algorithm to evaluate the quality of different subsets of features. They search for the combination of features that results in the best model performance, but they can be computationally expensive.
*   **Embedded Methods**: These methods perform feature selection as part of the model training process itself. A classic example is L1 regularization (Lasso), which adds a penalty to the model's loss function based on the absolute value of the feature weights, effectively shrinking the weights of less important features to zero.

### Feature Transformation

Feature transformation, or dimensionality reduction, is the process of transforming data from a high-dimensional space into a lower-dimensional space, while preserving some meaningful properties of the original data. This can help to visualize the data, reduce computational complexity, and remove noise.

<Figure
  src={pcaIca}
  alt="PCA and ICA"
  caption="PCA and ICA"
/>

*   **Principal Component Analysis (PCA)**: PCA is a linear technique that finds a new set of orthogonal axes, called principal components, that capture the maximum amount of variance in the data. The data is then projected onto a smaller number of these components.
*   **Independent Component Analysis (ICA)**: ICA is another linear technique that separates a multivariate signal into additive, non-Gaussian subcomponents. It is particularly useful for separating mixed signals, like separating individual voices from a mixed audio recording.
*   **Linear Discriminant Analysis (LDA)**: LDA is a supervised dimensionality reduction technique that finds the feature subspace that maximizes the separability between the classes. It is often used as a pre-processing step for classification tasks.

<Figure
  src={tsneUmap}
  alt="t-SNE and UMAP"
  caption="t-SNE and UMAP"
/>


*   **t-SNE (t-Distributed Stochastic Neighbor Embedding)**: t-SNE is a non-linear technique that is particularly well-suited for visualizing high-dimensional datasets. It models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability.
*   **UMAP (Uniform Manifold Approximation and Projection)**: UMAP is another non-linear technique for manifold learning and dimensionality reduction. It is often faster than t-SNE and can better preserve the global structure of the data.


### Information Theory

Information theory is a mathematical framework for quantifying information. In the context of machine learning, it provides tools for measuring the amount of uncertainty and information in data.

The most fundamental concept is **entropy**, which measures the average level of "surprise" or "uncertainty" inherent in a variable's possible outcomes. In the context of decision trees, we use **information gain**, which is the reduction in entropy achieved by splitting the data on a particular feature. By always choosing the split that maximizes information gain, we can build a more efficient decision tree. Information theory also provides the foundation for other concepts like cross-entropy, which is a common loss function for classification problems, and mutual information, which can be used for feature selection.

## Reinforcement Learning
{/* TODO: Add content for Reinforcement Learning */}

### Markov Decision Processes

A Markov Decision Process (MDP) is a mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision-maker. It is the formal foundation for reinforcement learning.

<Figure
  src={mdp}
  alt="Markov Decision Process"
  caption="Markov Decision Process"
/>

An MDP is defined by:
*   A set of **states** ($S$)
*   A set of **actions** ($A$)
*   A **transition model** $T(s, a, s')$, which gives the probability of transitioning from state $s$ to state $s'$ after taking action $a$.
*   A **reward function** $R(s, a, s')$, which gives the immediate reward received after transitioning from state $s$ to $s'$ as a result of action $a$.

The goal in an MDP is to find a **policy** $\pi(s)$, which is a mapping from states to actions, that maximizes the cumulative expected reward over time.

### Reinforcement Learning

Reinforcement Learning (RL) is an area of machine learning concerned with how an **agent** ought to take **actions** in an **environment** in order to maximize some notion of cumulative **reward**. RL algorithms learn a policy by trial and error, interacting with the environment and receiving feedback in the form of rewards or punishments.

<Figure
  src={rlModels}
  alt="Reinforcement Learning Models"
  caption="Reinforcement Learning Models"
/>

There are two main approaches to solving RL problems:
*   **Model-Based RL**: In this approach, the agent first tries to learn the transition model and reward function of the environment (i.e., learn the MDP). Once the model is learned, the agent can use planning algorithms (like value iteration or policy iteration) to find the optimal policy.
*   **Model-Free RL**: In this approach, the agent learns a policy directly without explicitly learning the environment's model. This is useful when the environment is too complex to model. Popular model-free algorithms include:
    *   **Q-Learning**: A value-based method that learns the optimal action-value function (Q-function), which represents the expected cumulative reward of taking a certain action in a certain state.
    *   **Policy Gradient Methods**: These methods learn the policy directly by adjusting its parameters in the direction that increases the expected reward.

### Game Theory

Game theory is the study of mathematical models of strategic interaction among rational decision-makers. In the context of machine learning, it is particularly relevant for multi-agent reinforcement learning, where multiple agents are interacting in the same environment.

Key concepts in game theory include:
*   **Players**: The decision-makers in the game.
*   **Strategies**: The possible actions that each player can take.
*   **Payoffs**: The outcomes or rewards that each player receives for a given combination of strategies.
*   **Nash Equilibrium**: A state of the game where no player has an incentive to unilaterally change their strategy, given the strategies of the other players.

<Figure
  src={nash}
  alt="Nash Equilibrium"
  caption="Nash Equilibrium"
/>

By modeling multi-agent problems as games, we can analyze the dynamics of competition and cooperation and design agents that can learn to behave strategically in the presence of other intelligent agents.

## Course Overview

The projects were a great way to solidify the concepts learned in the course and took up a significant portion of the time I spent on the course.

Here are the projects from the course:
- **Project 1:** Supervised Learning
- **Project 2:** Randomized Optimization
- **Project 3:** Unsupervised Learning & Dimensionality Reduction
- **Project 4:** Reinforcement Learning

I'm planning to write deeper dives on each of the projects.

### Grading

Here's the grading breakdown from when I took the course in Spring 2024, based on the provided notes:

| Component | Weight |
|-----------|--------|
| Project 1 | 12.5%  |
| Project 2 | 12.5%  |
| Project 3 | 12.5%  |
| Project 4 | 12.5%  |
| Midterm Exam | 25%    |
| Final Exam | 25%    |


### Books

These are some classic books in Machine Learning that complement the course material.

{/* ML Mitchell, Hands-On ML, DS Design Manual, Intro to Stat Learning, AI Modern Approach, Grokking Deep RL */}

<BookCard
  title="Machine Learning"
  author="Tom Mitchell"
  img={mlMitchell}
>
  <p>
    A foundational text that provides a comprehensive introduction to machine learning. This is the core textbook for the course.
  </p>
</BookCard>

<BookCard
  title="An Introduction to Statistical Learning"
  author="Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani"
  img={introToStatLearning}
>
  <p>
    A great book for getting started with machine learning. It covers a lot of the same material as the course, but in a more statistical perspective. It's less rigorous than The Elements of Statistical Learning, but it's a great book for getting started.
  </p>
</BookCard>

<BookCard
  title="Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow"
  author="Aurélien Géron"
  img={handsOnMl}
>
  <p>
    A hands-on guide to machine learning with Python. This is a great book for getting started with machine learning. This was particularly helpful for the first project.
  </p>
</BookCard>

<BookCard
  title="The Data Science Design Manual"
  author="Steve Skiena"
  img={dsDesignManual}
>
  <p>
    Provides a comprehensive overview of the data science process, from data collection to model deployment. I thought the treatment of data analysis and visualization was particularly helpful.
  </p>
</BookCard>

<BookCard
  title="Artificial Intelligence: A Modern Approach (4th Ed)"
  author="Stuart Russell and Peter Norvig"
  img={aiModernApproach}
>
  <p>
    A classic, often considered a bible for AI. It covers a vast range of topics from a philosophical perspective. The chapters on search, random optimization, unsupervised learning, and reinforcement learning were particularly helpful for the course.
  </p>
</BookCard>

<BookCard
  title="Reinforcement Learning: An Introduction (2nd Ed)"
  author="Richard S. Sutton and Andrew G. Barto"
  img={rlSuttonBarto}
>
    <p>
        The standard textbook for reinforcement learning, covering the core concepts and algorithms.
    </p>
</BookCard>

<BookCard
  title="Grokking Deep Reinforcement Learning"
  author="Miguel Morales"
  img={grokkingDeepRL}
>
  <p>
    A great book for getting started with reinforcement learning. It covers a lot of the same material as the course, but in a more mathematical perspective. This was particularly helpful for the fourth project.
  </p>
</BookCard>

### Other Resources

Here are some other posts I found helpful:
{/* TODO: Add links to other resources */}

---

## My Experience

{/* TODO: Add personal experience with the course */}

Thanks for reading!

## References
{/* TODO: Add references */}