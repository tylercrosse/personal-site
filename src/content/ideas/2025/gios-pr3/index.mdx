---
title: 'Proxy & Cache Server'
description: 'Building a Proxy and Cache Server with IPC and Shared Memory in C: Lessons from a Graduate OS Course'
date: '2025-06-24'
status: 'in-progress'
type: 'project'
tags: ['GIOS', 'operating-systems', 'c', 'multithreading', 'IPC', 'shared-memory']
category: ['projects']
draft: false
audience: 'All'
media_subpath: "/ideas/gios-pr3/"
image:
  path: "./moebius-fly-dawn.png"
  alt: 'Moebius Fly Night by Jean Giraud'
---


## Introduction

Inter-process communication (IPC) is a cornerstone of systems programming, enabling processes to share information and coordinate their actions. As part of a graduate Operating Systems course, I developed a system comprising a proxy server and a cache server that communicate using shared memory. This project provided valuable insights into designing scalable systems and leveraging IPC mechanisms effectively.

In this post, I'll share key lessons from the experience, focusing on high-level concepts, challenges, and the broader implications of building such systems.

## Overview and Objectives

The project extended an existing file server by introducing two components:

1. **Proxy Server**: Acts as an intermediary, translating client requests into web server requests and relaying responses.
2. **Cache Server**: Stores frequently requested data to improve efficiency and reduce response times, communicating with the proxy via shared memory.

This system emulates real-world scenarios where performance and resource management are critical. The proxy handles external communication while the cache optimizes repeated requests. Both components required robust inter-process coordination and synchronization.

### Key Challenges

- **Handling Concurrency**: Ensuring smooth operation with multiple clients and processes.
- **Designing IPC Mechanisms**: Efficiently transferring data between the proxy and the cache.
- **Resource Management**: Cleaning up shared memory and other IPC resources to prevent leaks or conflicts.

## Designing the System

The system architecture emphasized modularity and scalability:

1. **Proxy Server**:
   - Serves as the interface between clients and the web or cache server.
   - Uses multithreading to handle concurrent client requests efficiently.
   - Employs a boss-worker model to distribute workload among threads.

2. **Cache Server**:
   - Acts as a centralized store for frequently accessed files.
   - Communicates with the proxy using shared memory for data transfer and a command channel for coordination.
   - Implements synchronization mechanisms to ensure safe access to shared resources.

### High-Level Design Choices

- **Shared Memory for Data Transfer**: Chosen for its performance benefits, allowing large file contents to be shared without redundant copying.
- **Command Channel for Coordination**: Used to transmit metadata and control messages between the proxy and cache, ensuring clear separation of concerns.
- **Signal Handling for Cleanup**: Ensured proper resource management by intercepting termination signals to release shared memory and other IPC resources.

## Key Challenges and Insights

### Concurrency and Synchronization

Building a multithreaded system required careful handling of shared resources. Synchronization primitives like mutexes and condition variables were critical for preventing race conditions and ensuring thread safety. The experience reinforced the importance of:

- Designing thread-safe data structures.
- Minimizing contention to maintain high performance.
- Testing for edge cases, such as deadlocks and resource starvation.

### IPC Design and Implementation

Using shared memory and message queues highlighted the trade-offs between performance and complexity. While shared memory provided high-speed data transfer, it required precise coordination to avoid inconsistencies. Lessons learned include:

- The value of clear communication protocols between processes.
- The need for robust error handling in IPC mechanisms.
- Balancing simplicity and functionality in system design.

### Resource Management

Proper cleanup of IPC resources was essential for preventing memory leaks and ensuring system stability. Implementing signal handlers to release resources upon process termination was a critical step in achieving reliability.

## Broader Implications

This project underscored the real-world relevance of IPC and multithreading in building efficient, scalable systems. From web servers to distributed databases, the concepts explored here apply to numerous domains where performance and concurrency are paramount.

Key takeaways include:

- The importance of modular design in managing complexity.
- The trade-offs between different IPC mechanisms.
- The critical role of testing and debugging in developing robust systems.

## Conclusion

Developing a proxy and cache server with shared memory was a challenging yet rewarding experience. It provided a deeper understanding of systems programming concepts and practical insights into designing scalable and efficient solutions.

For those interested in exploring IPC, projects like this offer an excellent opportunity to build foundational skills. Whether working on simple file servers or complex distributed systems, the principles of concurrency, synchronization, and resource management remain universal.

## Additional resources

- [GIOS Course Website](https://omscs.gatech.edu/cs-6200-introduction-operating-systems)
- [K&R C Programming Language](https://en.wikipedia.org/wiki/The_C_Programming_Language)
- [Beej's Guide to Network Programming](https://beej.us/guide/bgnet/)
- [Beej's Guide to C Programming](https://beej.us/guide/bgc/)
- [Beej's Guide to Unix Socket Programming](https://beej.us/guide/bgnet/)
- [Linux Programming Interface](https://man7.org/tlpi/)
- [Operating Systems: Three Easy Pieces](https://pages.cs.wisc.edu/~remzi/OSTEP/)
