---
title: 'IPC in Action: Building a High-Performance Proxy & Cache'
description: 'A dive into inter-process communication, shared memory, and synchronization in C, based on a project from my graduate Operating Systems course.'
date: '2025-06-24'
status: 'complete'
type: 'project'
tags: ['GIOS', 'operating-systems', 'c', 'IPC', 'socket-programming']
category: ['projects']
draft: false
audience: 'All'
media_subpath: "/ideas/gios-pr3/"
image:
  path: "./pizza-cache2.png"
  alt: 'Pizza Cache'
---

import { Image } from "astro:assets";
import BookCard from "../../../../components/BookCard.astro";
import Figure from "../../../../components/Figure.astro";

import osTepImg from "../../../books/os-tep.jpg";
import csAppImg from "../../../books/cs-app.jpg";
import cLangImg from "../../../books/c-lang.jpg";
import beejGuidesImg from "../../../books/beej-network.jpg";
import linuxInterfaceImg from "../../../books/linux-interface.png";

import ipcImg from "./ipc.png";
import stringCommImg from "./string-communication.jpg";
import proxyCacheFlowImg from "./proxy-cache-flow.png";
import proxyCacheProcessesImg from "./proxy-cache-processes.png";
import ipcMechanismsImg from "./ipc-mechanisms.png";
import placeholderCommunicationImg from "./placeholder-communication.png";
import placeholderLinuxCommunicationOverviewImg from "./placeholder-linux-communication-overview.png";
import placeholderLinuxSynchronizationImg from "./placeholder-linux-synchronization.png";
import placeholderSharedMemoryImg from "./placeholder-shared-memory.png";

## Introduction: The Scalability Problem

Imagine a wildly popular news website. On a big story day, millions of users are all trying to load the same front-page article and its images. The main web server, buried deep in a datacenter, gets hammered with identical requests, spending its cycles reading the same files from disk over and over. For users on the other side of the world, the experience is slow because data must travel thousands of miles. This is a classic scalability bottleneck. How do you serve popular content to a global audience, quickly and efficiently, without overwhelming your origin server?

The solution is a one-two punch of architectural patterns: the **cache server** and the **proxy server**.

A **cache server** is a simple but powerful idea: it's a separate, specialized server that keeps temporary copies of frequently requested data. Instead of the main server handling every request, the cache can serve up a copy of that popular news article instantly, from memory. This dramatically reduces the load on the origin and provides a much faster response.

But how do client requests get directed to the cache instead of the main server? That's the job of a **proxy server**. A proxy acts as an intelligent intermediary, a gatekeeper that sits between the client and the internet. When a request comes in, the proxy intercepts it. It can then make a smart decision: "Do I have a fresh copy of this in a nearby cache?" If so, it serves it directly. If not, *then* it forwards the request to the main origin server, gets the response, and passes it back to the clientâ€”while also telling the cache to store a copy for next time.

<Figure
  src={proxyCacheFlowImg}
  alt="A diagram showing a client talking to a proxy, which in turn communicates with a cache server via IPC and a web server via HTTP."
  caption="The proxy and cache are separate processes, communicating via dedicated IPC channels."
/>

For my third project in Georgia Tech's OS course, the challenge was to build exactly this system: a cooperating proxy and cache. This project wasn't about threads within a single program; it was about two distinct processes that had to communicate and coordinate. This forced a deep dive into one of the most fundamental topics in operating systems: **Inter-Process Communication (IPC)**. How could we pass massive amounts of file data from a cache process to a proxy process with minimal overhead? This post explores that journey, and the powerful, tricky, and essential world of IPC that makes systems like this possible.

## The Architecture: A Tale of Two Processes

In the real world, the line between a proxy and a cache is often blurry. It's crucial to understand their distinct roles and how this project intentionally separates them to create a specific engineering challenge.

### The Modern Proxy: A Versatile Gatekeeper

A proxy server is an intermediary that sits between a client and a server. As [Wikipedia explains](https://en.wikipedia.org/wiki/Proxy_server), there are two main types:

* **Forward Proxy:** Acts on behalf of a *client* or a network of clients. When you use a proxy at a company or school to access the internet, you're using a forward proxy. It can be used to enforce security policies, filter content, or mask the client's identity.
* **Reverse Proxy:** Acts on behalf of a *server* or a pool of servers. When you connect to a major website, you're almost certainly talking to a reverse proxy. It presents a single, unified entry point to a complex system of backend services.

Reverse proxies are the workhorses of the modern web, often handling multiple critical tasks beyond just forwarding requests:
- **Load Balancing:** Distributing incoming traffic across many backend servers.
- **SSL Termination:** Decrypting incoming HTTPS traffic so backend servers don't have to.
- **Authentication:** Verifying user credentials before passing a request to a protected service.
- **Caching:** Storing copies of responses to speed up future requests, a role so common that the term "caching proxy" is widely used [[1]](https://web.archive.org/web/20250527171714/https://www.techtarget.com/whatis/definition/cache-server).

### The Project's Design: An Intentional Separation

In a production system like Nginx or a Content Delivery Network (CDN), the proxy and cache logic are typically integrated into the same highly-optimized application. For this project, however, the two were explicitly separated into distinct processes to create a focused challenge in systems programming.

1.  **The Proxy Process**: This acts as a simple reverse proxy. Its only jobs are to intercept client requests, ask the cache if it has the file, and if not, use `libcurl` to fetch it from the origin server. It handles the network I/O and protocol logic.
2.  **The Cache Process**: This is a pure, specialized key-value store. Its only job is to store and retrieve file data from memory. It doesn't know about `GETFILE` or HTTP; it only responds to commands from the proxy.

This artificial separation was the heart of the project. By splitting these tightly-coupled roles, the primary engineering problem became: **how to build a high-bandwidth, low-latency communication channel between them?** This forces a deep dive into Inter-Process Communication, making it the central lesson.

<Figure
  src={proxyCacheProcessesImg}
  alt="A diagram showing the proxy and cache processes and their communication."
  caption="The proxy and cache processes and their communication."
/>

## A Primer on Inter-Process Communication (IPC)

Modern operating systems are built on a bedrock principle: **process isolation**. Each process gets its own private virtual address space, its own chunk of memory that no other process can touch. This is a crucial security and stability feature. Without it, a bug in your web browser could crash your entire operating system.

But this isolation creates a problem: what if processes *need* to cooperate? That's where IPC comes in. The OS provides a set of controlled "doorways" through the walls of process isolation. The kernel mediates these mechanisms, ensuring that communication happens in a structured and safe way.

Some common IPC mechanisms include:

- **Pipes:** A simple one-way communication channel. What you write in one end, you read from the other.
- **Message Queues:** A structured way to send small "messages" between processes, like a system-wide mailbox.
- **Shared Memory:** The fastest, most powerful, and most dangerous method. The OS maps the same region of physical RAM into the virtual address space of multiple processes, allowing them to read and write to the same memory location directly.
- **Semaphores:** A synchronization primitive that allows processes to coordinate their access to shared resources.

<Figure
  src={ipcMechanismsImg}
  alt="A diagram showing the different IPC mechanisms and their use cases."
  caption="The different IPC mechanisms and their use cases."
/>

For this project, the goal was maximum performance, which pointed to a hybrid approach: **Shared Memory** for the heavy lifting (transferring file data) and a **Message Queue** for coordination.

The project mandated a strict division of this communication:
- A high-speed **data channel** for transferring the large file contents.
- A separate **command channel** for control messages like "do you have this file?"

## Shared Memory: The High-Speed Data Link

Shared memory is the IPC equivalent of teleportation. Instead of packaging data, handing it to the kernel (a system call), and having the kernel deliver it to another process (another system call), shared memory bypasses the kernel entirely for the data transfer itself.

Once set up, a write to a memory address in process A is *instantly* visible to process B at a corresponding memory address. This is incredibly fast because there's no copying. Both processes are looking at the *exact same bytes* in physical RAM.

But this power comes with immense responsibility.

- **No Built-in Synchronization:** Shared memory is just a raw slab of bytes. If the proxy is reading from a segment of memory while the cache is still writing to it, the proxy will read corrupted, nonsensical data. There are no traffic lights or stop signs; it's a free-for-all.
- **Life After Death:** IPC resources like shared memory segments exist at the OS level, outside of any single process. If a program crashes without cleaning up, it can orphan the shared memory segment, leaking system resources until the next reboot.

<Figure
  src={placeholderSharedMemoryImg}
  alt="A diagram showing the different shared memory mechanisms and their use cases."
  caption="The different shared memory mechanisms and their use cases."
/>

## Message Queues: The Command Channel

If shared memory is the bulk cargo freighter, the message queue is the walkie-talkie used to coordinate the loading and unloading. We needed a way for the proxy to send commands to the cache, like "Do you have `/images/cat.jpg`?" or "Please store this file I just downloaded."

A message queue is perfect for this. It allows for sending small, structured messages between processes. The proxy could package up a request into a message, send it to the queue, and the cache, which would be listening on that queue, would receive it and know what to do. This neatly separates the control signals from the data itself. The messages contained metadata about the files, while the shared memory was reserved for the file contents.

## Design in Practice: Synchronization and Robustness

With the core components defined, the real challenge lay in making them work together reliably. This involved not just preventing data corruption, but also designing the system to handle the realities of process lifecycle and startup.

### Preventing Chaos with Semaphores

To prevent the proxy and cache from writing over each other's data in the shared memory, it needed a synchronization primitive that works across processes. While mutexes are great for threads, **semaphores** are a classic solution for synchronizing separate processes.

<Figure
  src={placeholderCommunicationImg}
  alt="A diagram showing a semaphore and its use in synchronization."
  caption="A semaphore and its use in synchronization."
/>

A semaphore acts as a gatekeeper for a resource. Before accessing the shared memory, a process must acquire the semaphore. If the semaphore is already taken, the process will block (sleep) until it's released. This ensures that only one process can be operating on the shared data at any given time, preventing data races and corruption. It's the traffic light system we needed for our shared memory highway.

> [!note]
> Semaphores are just one of many tools for synchronization. For a broader look at how they compare to thread-focused tools like mutexes or high-performance spinlocks, see my overview of [Synchronization Constructs in the GIOS Retrospective](/ideas/2025/gios-retro/#synchronization-constructs).

### Responsibility and Resource Management

A key design decision mandated by the project was that **the proxy would be responsible for creating and destroying all IPC resources**. From the cache's perspective, this design respects its memory boundaries. It makes the cache a more robust service, capable of handling multiple proxy clients without being brought down by a single misbehaving one. If a proxy crashes, it's responsible for its own mess, and the cache can continue serving other clients.

This responsibility extends to cleanup. It's not enough for the system to work; it must also exit cleanly. The project required implementing **signal handlers** to catch termination signals like `SIGINT` (from Ctrl-C) and `SIGTERM` (from a `kill` command). When caught, these signals trigger a cleanup routine that meticulously removes all shared memory segments and message queues before the process terminates, preventing orphaned resources from littering the OS.

### Flexible Startup

Finally, the system had to be robust to the order in which the processes were started. The proxy could launch before the cache was ready, or vice-versa. This meant the connection logic couldn't be a one-time attempt. The proxy, upon starting, had to be prepared to repeatedly attempt a connection to the cache's command channel if it wasn't yet available. This small detail is a classic example of the kind of defensive programming required when building distributed or multi-process systems.

## Conclusion: Building Cooperative Systems

This project was a fantastic lesson in the trade-offs of system design. While building a single, monolithic program is often simpler, splitting a system into specialized processes can lead to a more modular, scalable, and maintainable architecture.

The key takeaways were:

- **IPC is a spectrum:** There's a tool for every job, from slow-but-simple pipes to fast-but-complex shared memory. The right choice depends entirely on the problem's requirements.
- **Synchronization is non-negotiable:** With great power comes great responsibility. Using high-performance IPC like shared memory *requires* robust, process-aware synchronization to prevent chaos.
- **Resource management is paramount:** Processes are mortal, but OS-level resources can be eternal. Writing code that cleans up after itself is a hallmark of a reliable system.

While a production cache might use more advanced techniques, the fundamental principles are the same. This project provided a tangible, low-level understanding of how independent programs can be orchestrated to build a cooperative, high-performance system.

## Additional resources

These books and guides were extremely helpful for understanding the concepts and APIs for IPC in C.

<BookCard
  title="The C Programming Language"
  author="Brian W. Kernighan and Dennis M. Ritchie"
  img={cLangImg}
  url="https://www.google.com/books/edition/The_C_Programming_Language/OpJ_0zpF7jIC"
>
    <p>
        The definitive book on C, written by its creators. Essential for mastering the language itself.
    </p>
</BookCard>

<BookCard
  title="Beej's Guides to C, Network Programming, and IPC"
  author="Brian 'Beej' Hall"
  img={beejGuidesImg}
  url="https://beej.us/guide/"
>
    <p>
        An invaluable, practical, and free resource. The IPC guide was a lifesaver for this project, providing clear, working examples for shared memory and semaphores.
    </p>
</BookCard>

<BookCard
  title="The Linux Programming Interface"
  author="Michael Kerrisk"
  img={linuxInterfaceImg}
  url="https://man7.org/tlpi/"
>
    <p>
        The encyclopedic guide to the Linux and UNIX system programming interface. The chapters on System V IPC and POSIX IPC are incredibly detailed and authoritative.
    </p>
</BookCard>

<BookCard
  title="Operating Systems: Three Easy Pieces"
  author="Remzi H. Arpaci-Dusseau and Andrea C. Arpaci-Dusseau"
  img={osTepImg}
  url="https://pages.cs.wisc.edu/~remzi/OSTEP/"
>
  <p>
    This is the main book I used to supplement the lectures. Its chapters on concurrency and persistence provide the conceptual backbone for understanding why IPC is necessary and how it works.
  </p>
</BookCard>

> [!info] A Note on Code Availability
> In accordance with Georgia Tech's academic integrity policy and the license for course materials, the source code for this project is kept in a private repository. I believe passionately in sharing knowledge, but I also firmly respect the university's policies. This document follows [Dean Joyner's advice on sharing projects](https://www.reddit.com/r/OMSCS/comments/zwdwns/comment/j1udv6w/).
>
> I would be delighted to discuss the implementation details, architecture, or specific code sections in an interview. Please feel free to reach out to request private access to the repository.
