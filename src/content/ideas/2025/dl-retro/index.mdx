---
title: 'Deep Learning'
description: 'A survey of concepts covered in my graduate Deep Learning course'
date: '2025-10-20'
updated: '2025-10-20'
status: 'in-progress'
type: 'retro'
tags: ['deep-learning', 'python', 'pytorch', 'neural-networks', 'computer-vision', 'NLP', 'computer-science', 'learning-in-public', 'MSCS']
category: ['mscs']
draft: true
audience: 'All'
media_subpath: "/ideas/dl-retro/"
image:
  path: "./hero-green-ripples.png"
  alt: 'Green Ripples'
hideCaption: true
---

import BookCard from "../../../../components/BookCard.astro";
import Figure from "../../../../components/Figure.astro";

import mathematicsForMachineLearning from "../../../books/math-for-ml.jpg";
import elementaryLinearAlgebra from "../../../books/elementary-linear-algebra.jpg";
import deepLearningGoodfellow from "../../../books/dl-goodfellow.jpg";
import deepLearningPrince from "../../../books/dl-prince.jpg";
import handsOnMl from "../../../books/hands-on-ml.jpg";

import computationalGraph from "./computational-graph-placeholder.png";
import backprop from "./backprop-placeholder.png";
import cnn from "./convolution-placeholder.png";
// import transformer from "./transformer-placeholder.png";
// import gan from "./gan-placeholder.png";


This piece serves as both a personal retrospective and a technical overview. I took Georgia Tech's [CS 7643: Deep Learning](https://omscs.gatech.edu/cs-7643-deep-learning) in the Spring 2025 semester. I'm revisiting the material to consolidate my learning - following the Feynman technique of teaching to understand, while also sharing insights for those interested in the course or my background. My audience includes prospective OMSCS students, potential employers, and colleagues seeking to understand the course content and my current knowledge. The bulk of this article summarizes the course material, with additional sections covering course logistics, my personal experience, and helpful resources.

## Introduction to Deep Learning

Deep learning has become the driving force behind the recent explosion in AI capabilities, from image recognition to natural language understanding. This course provides a comprehensive tour of the field, starting with the foundational principles of neural networks and progressing through the advanced architectures that power today's most sophisticated models.

The course is structured into four main modules, each building on the last:
1.  **Introduction to Neural Networks**: The fundamentals of neural networks, from linear classifiers to backpropagation.
2.  **Convolutional Neural Networks (CNNs)**: The workhorses of computer vision.
3.  **Structured Neural Representations**: Handling sequential data with RNNs, LSTMs, and the revolutionary Transformer architecture.
4.  **Advanced Topics**: A look at the cutting edge, including Deep Reinforcement Learning and Generative Models.

We'll look at each module in turn, connecting the lecture concepts to the hands-on assignments that form the core of the learning experience.

## Module 1: Introduction to Neural Networks

This module lays the groundwork for the entire course, starting from first principles. We formalize supervised learning as the minimization of a loss functional

$$
\min_{\theta} \mathcal{L}(\theta) = -\frac{1}{N} \sum_{i=1}^N \left[ y_i \log \hat{y}_i + (1-y_i) \log (1-\hat{y}_i) \right],
$$

and then study how gradient descent iteratively updates the parameters by

$$
\theta^{(t+1)} = \theta^{(t)} - \eta \nabla_\theta \mathcal{L}(\theta^{(t)}).
$$

From there, we build up to a full understanding of a "vanilla" neural network. The key concept is the **computational graph**, which represents the network's operations as a directed graph so we can apply the chain rule efficiently—**backpropagation** is simply reverse-mode automatic differentiation on this graph.

<Figure
  src={computationalGraph}
  alt="Computational Graph"
  caption="A computational graph illustrates how inputs are transformed by a series of operations to produce an output, enabling efficient backpropagation. Credit Justin Johnson."
/>

<Figure
  src={backprop}
  alt="Backpropagation"
  caption="Backpropagation is a technique for efficiently computing the gradients of a neural network. Credit Justin Johnson."
/>

**Assignment 1** solidifies these concepts by having you build a neural network from scratch using only NumPy. This includes implementing:
*   Forward and backward passes for various layers (e.g., ReLU, Softmax).
*   Loss functions like Cross-Entropy.
*   An optimizer using Stochastic Gradient Descent (SGD).

This hands-on experience provides a deep intuition for how neural networks learn, before moving on to higher-level frameworks like PyTorch.

## Module 2: Convolutional Neural Networks

This module dives into Convolutional Neural Networks (CNNs), the architecture that revolutionized computer vision. We learn about the key building blocks: **convolutional layers**, which apply learnable filters to input images to detect features like edges and textures, and **pooling layers**, which downsample the feature maps to reduce dimensionality and create translational invariance. The discrete convolution that drives each layer is written as

$$
(f * k)_{i,j} = \sum_m \sum_n f_{i+m, j+n}\, k_{m,n},
$$

so analyzing how receptive fields grow with depth feels much more concrete than when I first encountered CNNs in a purely code-first way. We explore classic CNN architectures and the techniques used to visualize what they've learned.

<Figure
  src={cnn}
  alt="Convolutional Neural Network"
  caption="A typical CNN architecture, showing the flow from input image through convolutional and pooling layers to the final classification. Credit Justin Johnson."
/>

**Assignment 2** is a two-part challenge. First, you implement the forward and backward passes for convolutional and max-pooling layers in NumPy, reinforcing the fundamentals. Then, you transition to PyTorch to build a more complex CNN, gaining familiarity with the framework that will be used for the rest of the course.

**Assignment 3** focuses on understanding what happens inside the "black box" of a CNN. You implement techniques to visualize the network's learned features and create adversarial examples to fool the model, providing insight into its strengths and weaknesses.

## Module 3: Structured Neural Representations

This module shifts focus from images to sequential data, like text and time series. We begin with **Recurrent Neural Networks (RNNs)** and their more powerful variant, **Long Short-Term Memory (LSTM)** networks, which are designed to handle long-range dependencies in data. A recurrent cell propagates information via

$$
h_t = \tanh (W_{hh} h_{t-1} + W_{xh} x_t + b_h),
$$

while LSTMs extend this equation with gating to maintain a long-term cell state. The highlight of this module is the **Transformer architecture**, introduced in the seminal paper "Attention Is All You Need." Transformers dispense with recurrence entirely and rely on a mechanism called **self-attention** to weigh the importance of different words in a sequence:

$$
\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V.
$$

This allows for massive parallelization and has made Transformers the state-of-the-art for nearly all NLP tasks. Compared with [yxlow's 2022 experience](https://lowyx.com/posts/gt-dl/)—which emphasized reinforcement learning earlier—our Spring 2025 section devoted **Project 3** entirely to RNNs, LSTMs, and Transformers so we could push sequence-to-sequence models further before touching policy gradients.

{/* <Figure
  src={transformer}
  alt="The Transformer Architecture"
  caption="The Transformer architecture, based on self-attention, has become the dominant model for natural language processing tasks."
/> */}

**Assignment 4** (my numbering) is an extensive project where you implement several sequence models:
*   RNN and LSTM networks.
*   A Sequence-to-Sequence (Seq2Seq) model with attention.
*   The full Transformer model (encoder and decoder).

The final part of the assignment involves training a model to translate German to English, comparing the performance of the traditional Seq2Seq model against the more advanced Transformer, mirroring the techniques documented in [yxlow's course notes](https://lowyx.com/posts/gt-dl-notes/) but with newer PyTorch primitives.

## Module 4: Advanced Topics

The final module provides a glimpse into the broader applications of deep learning and is where my experience diverged most from yxlow's recap. While the 2022 run concluded with reinforcement learning-heavy material, our **Project 4** centered on generative modeling for FashionMNIST: VAEs, GANs, and even a tiny diffusion model.

*   **Deep Reinforcement Learning**: We still covered the policy gradient and Q-learning families conceptually, but primarily as context for how deep nets integrate with classical RL objectives.
*   **Variational Autoencoders (VAEs)**: We implemented the evidence lower bound

    $$
    \mathcal{L}_{\text{ELBO}} = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{\text{KL}}(q_\phi(z|x) \parallel p(z)),
    $$

    which enforces both reconstruction fidelity and a structured latent prior.
*   **Generative Adversarial Networks (GANs)**: Training revolved around the minimax problem

    $$
    \min_G \max_D \mathbb{E}_{x \sim p_{\text{data}}}[\log D(x)] + \mathbb{E}_{z \sim p(z)}[\log(1-D(G(z)))].
    $$

    Implementing the alternating optimization loop demystified the stability issues everyone warns about.
*   **Diffusion Models**: We explored how iteratively denoising a sample follows the reweighted score-matching objective, with the forward process defined as

    $$
    q(x_t | x_{t-1}) = \sqrt{1-\beta_t}\, x_{t-1} + \sqrt{\beta_t}\, \epsilon,
    $$

    and learned networks predicting either $\epsilon$ or $x_0$.

{/* <Figure
  src={gan}
  alt="Generative Adversarial Network"
  caption="A GAN consists of a Generator that creates fake images and a Discriminator that tries to distinguish them from real images. Through this competition, both networks improve."
/> */}

## Assignment Writeups

For readers who prefer compact, assignment-by-assignment recaps, I pulled my writeups into focused posts:

1. [DL A1 — Backprop from Scratch](/ideas/2025/dl-a1/): NumPy-based MNIST pipelines, theory problems, and paper reviews.
2. [DL A2 — Practical CNNs and Imbalanced Data](/ideas/2025/dl-a2/): PyTorch CNNs on CIFAR-10 plus class-balanced focal loss experiments with ResNet-32.
3. [DL A3 — Sequence Models, Attention, and Translation](/ideas/2025/dl-a3/): LSTMs, Seq2Seq with cosine attention, and Transformers for German→English.
4. [DL A4 — VAEs, GANs, and Diffusion on FashionMNIST](/ideas/2025/dl-a4/): Generative modeling lab notes (VAE, GAN, DDPM) across MNIST/FashionMNIST.
5. [DL Final Project — Crosscoding Reasoning Upgrades](/ideas/2025/dl-final-project/): Fine-tuning Qwen2.5-0.5B-Instruct on s1k and comparing representations with Crosscoders.

## Group Project

The course culminates in a group project where students can apply their knowledge to a real-world problem. This could involve reproducing the results of a research paper, exploring a novel architecture, or participating in a Kaggle-style competition. A popular project is the "Hateful Memes Challenge," which requires building a multimodal model that can understand the combination of text and images to detect hate speech.

## Course Overview

The course is demanding and time-consuming, particularly the assignments. Access to a GPU is highly recommended for Assignments 2 and 4, as well as the group project. The course provides guidance on using services like Google Colab.

### Grading

The grading breakdown from the Spring 2022 semester was as follows:

| Component      | Weight |
|----------------|--------|
| Assignments (4)| 55%    |
| Quizzes (6)    | 20%    |
| Final Project  | 20%    |
| Participation  | 5%     |

### Books

These are some of the key texts that are highly relevant to the course material.

<BookCard
  title="Mathematics for Machine Learning"
  author="Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong"
  img={mathematicsForMachineLearning}
>
  <p>
    A comprehensive introduction to the mathematics of machine learning, covering the concepts covered in the course. It's an excellent reference for the mathematical concepts covered in the course.
  </p>
</BookCard>

<BookCard
  title="Elementary Linear Algebra"
  author="Howard Anton"
  img={elementaryLinearAlgebra}
>
  <p>
    A classic textbook on elementary linear algebra.
  </p>
</BookCard>

<BookCard
  title="Deep Learning"
  author="Ian Goodfellow, Yoshua Bengio, and Aaron Courville"
  img={deepLearningGoodfellow}
>
  <p>
    Often called the "bible" of deep learning, this book provides a comprehensive and rigorous mathematical treatment of the field. It's an excellent reference for the theoretical concepts covered in the course.
  </p>
</BookCard>

<BookCard
  title="Understanding Deep Learning"
  author="Simon J. D. Prince"
  img={deepLearningPrince}
>
  <p>
    A more accessible introduction to deep learning, this book provides a clear and intuitive explanation of the concepts covered in the course. It's an excellent reference for the practical concepts covered in the course.
  </p>
</BookCard>

### Other Resources

Here are some of the excellent resources from other students that I found helpful:

- [yxlow's Course Review](https://lowyx.com/posts/gt-dl/)
- [yxlow's Course Notes](https://lowyx.com/posts/gt-dl-notes/)
- [Monzer Saleh's Course Notes](https://monzersaleh.github.io/GeorgiaTech/CS7643_DeepLearning.html)

## My Experience

{/* TODO: Add personal experience here. */}

Thanks for reading!

## References
