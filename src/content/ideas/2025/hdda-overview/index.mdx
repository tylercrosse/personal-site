---
title: 'High-Dimensional Data Analytics'
description: 'A survey of techniques for learning from data with high dimensions and a low number of samples'
date: '2025-09-11'
status: 'draft'
type: 'project'
tags: ['HDDA', 'machine-learning', 'python']
category: ['projects']
draft: true
audience: 'All'
media_subpath: "/ideas/gios-pr3/"
image:
  path: "./lidar-pointcloud.webp"
  alt: 'A colorized point cloud from the Nuscenes dataset'
---

## Introduction

This course provides a comprehensive tour of modern techniques for analyzing high-dimensional data, where the number of features can be very large. The central philosophy is that even when data appears complex on the surface, it often possesses an underlying low-dimensional structure that can be discovered and leveraged.

The most important, cross-cutting ideas from the course are:

1. **Finding Low-Dimensional Structure in High-Dimensional Data:** This is the core challenge addressed throughout the course. Whether dealing with functions, images, or tensors, the primary goal is to move beyond the "curse of dimensionality" by identifying simpler, latent structures. This includes:
    * **Smoothness** in functional data.
    * **Sparsity** in signals and model coefficients.
    * **Low-Rank** structure in matrices and tensors.

2. **Regularization as the Primary Tool:** Regularization is the fundamental mechanism used to enforce these structural assumptions and control model complexity. By adding a penalty term to the optimization objective, we can guide the model towards a solution that is less prone to overfitting and more interpretable. The key penalties are:
    * **L2 Penalty (Ridge):** Promotes smaller coefficients, leading to smoother models and stability in the presence of correlated predictors.
    * **L1 Penalty (LASSO):** Promotes sparsity by forcing some coefficients to be exactly zero, which performs automatic variable selection.
    * **Nuclear Norm:** The convex relaxation of matrix rank, used to encourage low-rank solutions in problems like matrix completion.

3. **Decomposition as a Powerful Strategy for Analysis:** Many complex problems are made tractable by decomposing the data (or the model itself) into simpler, meaningful components. This theme appears in:
    * **Functional PCA (FPCA):** Decomposing functions into a mean and key modes of variation.
    * **Tensor Decomposition (CP & Tucker):** Breaking down multi-dimensional arrays into a set of factor matrices.
    * **Robust PCA & Sparse-Smooth Decomposition:** Separating data into a primary low-rank/smooth component and a sparse component of outliers or anomalies.

4. **Optimization as the Engine:** The course equips us with the optimization algorithms needed to fit these advanced models. The key concepts include:
    * The trade-off between simple, scalable **first-order methods** (like Gradient Descent and SGD) and powerful but expensive **second-order methods** (like Newton's method).
    * Specialized algorithms like **Proximal Gradient Descent**, **ADMM**, and **Coordinate Descent** are essential for solving problems with the non-differentiable penalties (e.g., L1-norm) that are crucial for sparsity.

5. **The Importance of Convexity:** Convex optimization problems are central because they guarantee that any found local minimum is also the global minimum. A recurring technique is **convex relaxation**, where a computationally hard (non-convex) problem (like minimizing L0-norm or rank) is replaced with its closest convex equivalent (minimizing L1-norm or nuclear norm), which can be solved efficiently.

## Summary of Module 1: Functional Data

This module introduces the analysis of high-dimensional data, with a specific focus on **functional data**, where each observation is a function (like a signal, curve, or image).

1. **Introduction to High-Dimensional Data:** The module begins by defining "Big Data" by its volume, velocity, and variety. It introduces high-dimensional data and the "curse of dimensionality," where the distance between data points increases in high dimensions, making statistical modeling and computation challenging. The solution proposed is to find and leverage low-dimensional structures within the data.

2. **From Global to Local Regression:** The lecture reviews standard linear regression and highlights the limitations of using global polynomial regression for functional data. These models lack flexibility for complex patterns and are sensitive to outliers at the boundaries. This motivates a shift towards local regression methods.

3. **Splines for Flexible Modeling:** Splines are introduced as a powerful technique for fitting flexible curves. They work by dividing the domain of the function into segments using **knots** and fitting separate, simpler polynomials (like cubic polynomials) in each segment.
    * **Truncated Power Basis:** A simple way to construct splines, but it can be numerically unstable.
    * **B-Splines:** A more computationally stable and efficient basis for splines due to its "local support" property, meaning each basis function is non-zero only over a small interval.
    * **Natural Splines:** A variation that constrains the function to be linear beyond the boundary knots, which helps reduce high variance at the edges of the data range.
    * **Smoothing Splines:** This method avoids the difficult problem of choosing the number and location of knots. It uses all data points as knots but adds a penalty term to the fitting objective. This penalty discourages "wiggly" functions, and a smoothing parameter (`lambda`) controls the trade-off between fitting the data closely (low bias) and having a smooth function (low variance). This parameter is typically tuned using cross-validation.

4. **Kernel Smoothers:** This is another local regression method that estimates the value of a function at a target point by computing a weighted average of nearby observations.
    * A **kernel function** determines the weights, giving more importance to closer points.
    * The **bandwidth** (`lambda`) is a tuning parameter that defines the size of the "neighborhood" to consider, controlling the smoothness of the fit.
    * **Local Polynomial Regression** is an extension that fits a simple polynomial locally, which can correct for bias, especially at the boundaries.

5. **Functional Principal Component Analysis (FPCA):** This is the primary technique for dimensionality reduction of functional data. It decomposes each function into a shared mean function plus a weighted sum of characteristic variation patterns, called **eigenfunctions**.
    * The weights, known as **FPC scores**, are unique for each function and serve as a low-dimensional set of features that capture the most important modes of variation in the data.
    * The process involves estimating the mean and covariance functions from the observed data (often using smoothing techniques) and then deriving the eigenfunctions and scores.
    * These low-dimensional FPC scores can then be used as inputs for standard machine learning tasks like classification or regression.

### Most Important Ideas

* **Functional Data:** The core concept is treating data not as individual points or vectors, but as entire functions or curves (e.g., time series, spectrograms).
* **Local vs. Global Methods:** For complex data, local methods like splines and kernel smoothers are more flexible and effective than global models like standard polynomial regression.
* **Bias-Variance Trade-off:** A central theme is managing the trade-off between creating a model that fits the data well (low bias) and one that is not overly complex and sensitive to noise (low variance).
* **B-Splines for Stability:** B-splines are the preferred basis for spline regression because they are more numerically stable than simpler alternatives like the truncated power basis.
* **Smoothing and Regularization:** To prevent overfitting in flexible models, smoothing is essential. This is achieved either through explicit penalty terms (as in smoothing splines) or by local averaging (as in kernel smoothers). This process requires tuning a smoothing parameter (`lambda` or bandwidth) that controls the model's complexity.
* **FPCA for Dimensionality Reduction:** Functional Principal Component Analysis (FPCA) is the key tool for reducing the dimensionality of functional data. It effectively summarizes each function with a small number of "FPC scores," which can then be used in downstream machine learning models.

## Summary of Module 2: Image Analysis

This module provides an introduction to the fundamentals of digital image processing. It covers how images are represented digitally and introduces key techniques for image transformation, filtering, segmentation, and edge detection.

1. **Image Representation and Basics:**
    * Images are represented as matrices where each element (**pixel**) corresponds to a light intensity.
    * Common types are **Grayscale** (a single matrix), **RGB color** (three matrices for Red, Green, and Blue channels), and **Black & White** (a binary matrix).
    * Basic operations include reading/writing images, changing their size (resolution), and converting between types.

2. **Image Transformation and Enhancement:**
    * The **histogram**, which shows the distribution of pixel intensities, is a key diagnostic tool for understanding an image's brightness and contrast.
    * **Brightness** can be adjusted by adding a constant to all pixel values (shifting the histogram).
    * **Contrast** can be adjusted by stretching or compressing the intensity range (stretching the histogram).
    * Functions like log and power-law transforms can be used for more advanced image enhancement.

3. **Image Filtering with Convolution:**
    * **Convolution** is the core operation for image filtering. It works by sliding a small matrix called a **kernel** (or mask) across the image to modify pixel values based on their neighbors.
    * The choice of kernel determines the filter's effect, such as **smoothing/denoising** (e.g., with an averaging filter) or **sharpening** to enhance details.

4. **Image Segmentation:**
    * The goal of segmentation is to partition an image into meaningful regions (e.g., separating an object from its background). This is conceptually the same as clustering.
    * **Otsu's Method** is an algorithm that automatically finds an optimal threshold from the image's histogram to separate pixels into two or more classes.
    * **K-Means Clustering** can be applied directly to the pixel color/intensity values to group them into a specified number of segments.

5. **Edge Detection:**
    * Edges are fundamental features that correspond to object boundaries and are identified by finding abrupt changes in pixel intensity.
    * Edge detection is typically done by approximating the **derivatives** of the image intensity.
    * The **Sobel operator** is a widely-used method that calculates the image gradient to find edges.
    * The **Laplacian of Gaussian (LoG)** operator is robust to noise because it first smooths the image before applying a second-derivative operator to find edges.

### Most Important Ideas

* **Image as a Matrix:** The core idea that an image is a grid of numbers that can be manipulated mathematically.
* **Convolution with Kernels:** The central mechanism for most filtering operations. The kernel's values define the filter's specific purpose (blur, sharpen, find edges, etc.).
* **Histogram as a Tool:** The histogram is a simple but powerful tool for analyzing an image's properties and guiding enhancement and segmentation strategies.
* **Segmentation is Clustering:** The task of partitioning an image is equivalent to clustering its pixels based on their features (like color and intensity).
* **Edges are Derivatives:** Sharp changes in an image correspond to large derivatives of the image intensity function.

## Summary of Module 3: Tensor Analysis

This module introduces tensors (multi-dimensional arrays) as a natural way to represent high-dimensional data like videos or image collections. It covers fundamental tensor algebra and focuses on two key decomposition methods, CP and Tucker, for dimensionality reduction and data analysis.

1. **Tensor Basics and Multilinear Algebra:**
    * Tensors are defined as multi-dimensional arrays (e.g., a scalar is a 0D tensor, a vector is 1D, a matrix is 2D).
    * Key terminology includes **order** (number of dimensions), **fibers** (generalizations of rows/columns), and **slices** (2D matrices within a tensor).
    * The module reviews fundamental operations like **inner/outer products**, **matricization** (unfolding a tensor into a matrix), and various specialized products like the **n-mode product**, **Kronecker product**, and **Khatri-Rao product**.

2. **CP (CANDECOMP/PARAFAC) Decomposition:**
    * This method decomposes a tensor into a sum of **rank-one tensors**. A rank-one tensor is simply the outer product of several vectors.
    * The minimal number of rank-one tensors required to perfectly represent the tensor is its **rank**.
    * CP is a powerful tool for **dimensionality reduction**, as it allows a large tensor to be represented by a few small **factor matrices**.
    * The decomposition is typically computed by minimizing reconstruction error using an **Alternating Least Squares (ALS)** algorithm.

3. **Tucker Decomposition:**
    * A more general and flexible decomposition method that breaks a tensor down into a smaller **core tensor** and a set of **factor matrices**, one for each mode.
    * The core tensor is a compressed version of the original tensor that captures the interactions between the modes.
    * Unlike CP, which has a single rank, Tucker is defined by a set of **n-ranks** (one for each mode), which allows for more modeling flexibility.
    * This decomposition is also typically solved using an **ALS** algorithm, often initialized with results from a **Higher-Order SVD (HOSVD)**.

4. **Tensor Regression:**
    * The module shows how tensor decomposition can be applied to regression problems where either the predictors or the response variable are tensors.
    * **Scalar Response, Tensor Predictor:** In a problem predicting a system's remaining lifetime (scalar) from an image stream (tensor), the high-dimensional tensor of regression coefficients is decomposed (using CP or Tucker) to make the model estimation feasible.
    * **Tensor Response, Scalar Predictors:** In a problem optimizing a manufacturing process where the output quality is a point cloud (tensor), the coefficient tensor is decomposed to model the relationship between the scalar inputs (e.g., speed, depth) and the tensor output.

### Most Important Ideas

* **Tensor as a Data Structure:** Tensors are the natural way to represent data with more than two modes, such as videos (height × width × time) or spatio-temporal datasets.
* **Decomposition for Dimensionality Reduction:** CP and Tucker decompositions are the key methods for finding low-dimensional structure in tensor data. They are analogous to matrix factorization techniques like SVD/PCA but generalized to higher dimensions.
* **CP vs. Tucker:**
    * **CP** is a stricter model that seeks to find a set of shared, rank-one components. It is governed by a single rank value and is useful for interpreting latent factors.
    * **Tucker** is more flexible, performing a compression of the tensor into a core tensor and orthogonal factor matrices. It is often better for general-purpose compression because it allows for different ranks along each mode.
* **Alternating Least Squares (ALS):** This is the workhorse iterative algorithm for fitting both CP and Tucker models. It works by optimizing for one factor matrix at a time while holding the others fixed.
* **Tensor Regression:** When faced with regression models involving high-dimensional data, directly estimating parameters is often impossible. By decomposing the coefficient tensor itself, tensor regression makes it possible to build these models with a manageable number of parameters.

## Summary of Module 4: Optimization Methods

This module covers fundamental optimization algorithms used in machine learning and data analysis, focusing on continuous, convex problems. It introduces first-order methods that use gradient information and second-order methods that use curvature (Hessian) information.

1. **Optimization Fundamentals:**
    * An optimization problem consists of finding the minimum or maximum of an **objective function**, subject to a set of **constraints**.
    * **Convex Optimization** is a class of problems where the objective function and the feasible set are both convex. This property is highly desirable because it guarantees that any **local minimum is also a global minimum**, making the problem much easier to solve reliably.
    * The convexity of a function can be verified using its first derivative or its second derivative (the **Hessian matrix**), which must be positive semidefinite.

2. **First-Order Methods (Gradient-Based):**
    * These methods use the first derivative (gradient) to iteratively find a solution by moving in the direction that decreases the function value.
    * **Gradient Descent:** The most fundamental algorithm, which takes steps in the direction of the **negative gradient** (the direction of steepest descent). The size of each step is determined by a **step size** parameter, which can be fixed or adapted using techniques like **backtracking line search**.
    * **Accelerated Gradient Descent:** An improvement on standard gradient descent (e.g., Nesterov's method) that uses a "momentum" term. This helps it converge faster, with a theoretical rate of \(1/k^2\) compared to \(1/k\) for the standard method.
    * **Stochastic Gradient Descent (SGD):** A highly efficient algorithm for large-scale problems where the objective is a sum over many data points (e.g., training a machine learning model). Instead of computing the full gradient over the entire dataset at each step, it uses a single data point (or a **mini-batch**) to get a noisy but fast estimate of the gradient. This makes each iteration much cheaper and often leads to faster overall convergence.

3. **Second-Order Methods (Hessian-Based):**
    * These methods use the second derivative (Hessian matrix) to incorporate information about the function's curvature, which allows them to take more direct and intelligent steps toward the minimum, resulting in much faster convergence.
    * **Newton's Method:** At each step, this method approximates the function with a quadratic function and jumps directly to that quadratic's minimum. It converges very quickly but requires computing and inverting the Hessian matrix, which is often too expensive for high-dimensional problems.
    * **Quasi-Newton Methods (e.g., BFGS):** These methods provide a compromise between first-order and second-order methods. They avoid calculating the Hessian directly and instead build up an approximation of it (or its inverse) at each step using information from the gradients.
    * **Gauss-Newton Method:** A specialization of Newton's method used for non-linear least squares problems. It simplifies the calculation by approximating the Hessian using the Jacobian matrix, which avoids computing second derivatives.

### Most Important Ideas

* **Convexity is Key:** Convex optimization problems are central to machine learning because any local optimum found is guaranteed to be the global optimum.
* **Gradient as Direction:** First-order methods are built on the simple but powerful idea of iteratively moving "downhill" by following the negative gradient.
* **The Cost-vs-Speed Trade-off:** There is a fundamental trade-off in optimization algorithms. First-order methods are computationally cheap per iteration but may require many iterations to converge. Second-order methods are very expensive per iteration but converge in very few steps. Quasi-Newton methods offer a practical balance.
* **SGD for Scale:** For large datasets, computing the full gradient is prohibitive. Stochastic Gradient Descent (SGD) is the go-to method because its per-iteration cost is independent of the dataset size, making it possible to train models on massive amounts of data.
* **Curvature for Faster Convergence:** Second-order methods use curvature information (the Hessian) to build a much better local model of the function, which is why they can converge much more rapidly than first-order methods.

## Summary of Module 5: Advanced Optimization Methods

This module delves into optimization algorithms designed for a specific class of problems common in modern statistics and machine learning, particularly those involving non-differentiable penalty terms (like in LASSO). These methods exploit the structure of the objective function to solve problems that standard gradient descent cannot.

1. **Proximal Gradient Descent:**
    * This algorithm is designed for objective functions that can be decomposed into two parts: a smooth, differentiable convex function (`g`) and a convex but potentially non-differentiable function (`h`).
    * The core idea is to combine a standard gradient descent step for the smooth part (`g`) with a **proximal operator** step for the non-smooth part (`h`). The proximal operator finds a point that stays close to the gradient step while also minimizing `h`.
    * This method is very effective for problems like **LASSO**, where the proximal operator for the L1-norm is a simple **soft-thresholding** function.
    * Like standard gradient descent, it can be sped up using **Nesterov's acceleration**.

2. **Augmented Lagrangian Method (ALM) and ADMM:**
    * These methods are designed for constrained optimization problems or when the proximal operator is difficult to compute.
    * **Augmented Lagrangian Method (ALM)** converts a constrained problem into a series of unconstrained problems by adding both a **Lagrangian multiplier** term and a quadratic penalty term (the "augmentation") to the objective function.
    * **Alternating Direction Method of Multipliers (ADMM)** is a powerful variation that is well-suited for problems where the objective function can be split across different variables (e.g., `f(x) + g(z)`), linked by a linear constraint (`Ax + Bz = c`). It solves the problem by alternating between minimizing for `x`, minimizing for `z`, and then updating the dual variable (the multiplier). This decomposition often makes hard problems tractable.

3. **Coordinate Descent:**
    * This algorithm is applicable when the non-smooth part of the objective function is **separable**, meaning it can be written as a sum of functions that each depend on only one coordinate (or one block of coordinates).
    * It works by iteratively optimizing the objective function along one coordinate (or block of coordinates) at a time, holding all other coordinates fixed.
    * For problems like LASSO, coordinate descent is often extremely fast and simple to implement, as the one-dimensional subproblem can be solved exactly with a soft-thresholding step.
    * **Block Coordinate Descent** is a generalization where coordinates are grouped into blocks and optimized together.

### Most Important Ideas

* **Decomposition is Powerful:** The central theme is that complex optimization problems can often be solved efficiently by decomposing them into simpler, manageable parts.
* **The Proximal Operator:** A generalization of the projection operator that is the key building block for handling non-differentiable functions. It forms the basis of proximal gradient descent.
* **Handling Non-Smoothness:** This module provides a toolbox for the non-differentiable functions that are essential for modern regularized models (e.g., L1-norm for sparsity).
    * Use **Proximal Gradient Descent** if the non-smooth part has a simple proximal operator.
    * Use **ADMM** if the problem can be split into subproblems with constraints.
    * Use **Coordinate Descent** if the non-smooth part is separable by coordinates.
* **ADMM for complex problems:** ADMM is a highly versatile and powerful "Swiss army knife" algorithm that can handle a wide range of constrained and composite optimization problems by breaking them down into smaller pieces.

## Summary of Module 6: Regularization

This module explores various regularization techniques used in regression to improve model performance, handle high-dimensional data, and perform variable selection. It covers methods that use L1 and L2 penalties to shrink coefficients and create sparse models.

1. **Ridge Regression (L2 Penalty):**
    * Introduces regularization to control model complexity and prevent overfitting by penalizing large coefficient values.
    * Ridge regression uses an **L2 penalty** (the sum of squared coefficients), which shrinks coefficients towards zero but does not set them exactly to zero.
    * It is particularly effective at handling **multicollinearity** (highly correlated predictors) by reducing the variance of the coefficient estimates.
    * It does not perform variable selection. The amount of shrinkage is controlled by a tuning parameter (`lambda`), typically chosen via cross-validation.

2. **LASSO (L1 Penalty):**
    * LASSO (Least Absolute Shrinkage and Selection Operator) uses an **L1 penalty** (the sum of the absolute values of the coefficients).
    * This penalty enables both shrinkage and **automatic variable selection**, as it can force some coefficients to become exactly zero. This creates sparse, more interpretable models.
    * The non-differentiable nature of the L1 norm at zero is what allows for variable selection.
    * For orthonormal predictors, LASSO has a simple closed-form solution known as **soft-thresholding**.

3. **Non-Negative Garrote and Adaptive LASSO:**
    * These methods address a limitation of LASSO, which applies the same amount of shrinkage to all coefficients regardless of their magnitude.
    * **Adaptive LASSO** introduces weights into the L1 penalty. The weights are typically inversely proportional to initial coefficient estimates (e.g., from OLS or Ridge).
    * This allows the model to apply less penalty to large, important coefficients and more penalty to small, potentially non-informative ones, leading to more consistent variable selection.

4. **Elastic Net:**
    * A hybrid method that combines the **L1 penalty of LASSO and the L2 penalty of Ridge**.
    * It is designed to enjoy the benefits of both: it performs variable selection like LASSO while also handling correlated predictors effectively and providing stability like Ridge.
    * It is particularly useful in high-dimensional settings where the number of predictors is greater than the number of samples (`p > n`), and it exhibits a **grouping effect**, tending to select or remove groups of correlated variables together.

5. **Group LASSO:**
    * An extension designed for problems where predictors are naturally organized into pre-defined groups (e.g., dummy variables representing a single categorical feature).
    * It applies a penalty that encourages sparsity at the **group level**, meaning it either keeps or removes an entire group of variables from the model.
    * It performs selection *between* groups but not *within* them.

### Most Important Ideas

* **Regularization for Simpler Models:** The central concept is adding a penalty term to the loss function to control model complexity, which helps prevent overfitting and improves the model's generalization performance.
* **The L1 vs. L2 Trade-off:** The choice between penalties is fundamental. **L2 (Ridge)** provides stable shrinkage for correlated data. **L1 (LASSO)** provides sparsity and automatic feature selection.
* **Sparsity for Interpretability:** By forcing some coefficients to be exactly zero, L1-based methods produce simpler, more interpretable models that highlight the most important predictors.
* **Adaptive Penalties for Consistency:** Adaptive LASSO improves on LASSO's variable selection by using weights to penalize large and small coefficients differently, which can lead to better statistical properties.
* **Elastic Net as a Hybrid:** Elastic Net offers a robust and versatile solution that combines the strengths of LASSO and Ridge, making it highly effective for datasets with many predictors, correlated features, and `p > n` scenarios.
* **Group-Level Feature Selection:** When features have a natural structure, Group LASSO provides a way to perform selection on entire blocks of variables, which is critical for model interpretation in cases like ANOVA or functional regression.

## Summary of Module 7: Regularization Applications

This module explores several advanced applications of regularization in modern data analysis, moving beyond standard regression to problems like signal recovery, matrix imputation, and robust dimensionality reduction. The core idea is to impose structural assumptions (like sparsity or low-rank) on the solution via regularization.

1. **Compressive Sensing:**
    * This technique aims to recover a high-dimensional signal from a surprisingly small number of measurements.
    * The key assumption is that the signal is **sparse**, either in its original domain or in a different basis (like Fourier or wavelet).
    * Instead of solving the intractable L0-norm problem (finding the sparsest solution), it is relaxed to a convex **L1-norm minimization** problem, which can be solved efficiently. This allows for simultaneous sampling and compression, a major departure from traditional methods.

2. **Matrix Completion:**
    * This addresses the problem of filling in missing entries in a data matrix (e.g., the Netflix challenge).
    * The key assumption is that the underlying complete matrix is **low-rank**.
    * The non-convex problem of minimizing the matrix rank is relaxed to a convex problem of minimizing the **nuclear norm** (the sum of the singular values).
    * The solution is often found using iterative algorithms like **Singular Value Thresholding (SVT)**, which shrinks the singular values at each step to enforce a low-rank structure.

3. **Robust Principal Component Analysis (PCA):**
    * Standard PCA is known to be highly sensitive to outliers. Robust PCA is designed to overcome this.
    * It assumes the data matrix can be decomposed into two parts: a **low-rank matrix** (the true underlying data) and a **sparse matrix** (representing gross outliers or errors).
    * The problem is formulated as finding this decomposition by minimizing a weighted sum of the **nuclear norm** (for the low-rank part) and the **L1 norm** (for the sparse part). This is often solved with the Augmented Lagrangian Multiplier (ALM) method.

4. **Sparse Smooth Decomposition (SSD):**
    * This method decomposes a signal into three components: a **smooth functional mean**, a set of **sparse features** (or anomalies), and noise.
    * It solves the issues of traditional two-step approaches (e.g., denoise then detect) where initial smoothing can blur the very features one wishes to find.
    * The model uses a regularized least-squares framework that combines an **L2 penalty** to enforce smoothness on the mean function and an **L1 penalty** to enforce sparsity on the features, solving for both simultaneously.

5. **Kernel Ridge Regression and RKHS:**
    * This technique extends ridge regression to model complex, **non-linear relationships**.
    * It uses the **"kernel trick,"** where the inner products in the ridge regression solution are replaced by a kernel function. This implicitly maps the data into a high-dimensional feature space without ever computing the mapping.
    * The method is rigorously defined within a **Reproducing Kernel Hilbert Space (RKHS)**, where the L2 penalty becomes a penalty on the function's norm in this space, naturally encouraging smoothness.

### Most Important Ideas

* **Sparsity and Low-Rank as Priors:** The central theme is that many high-dimensional data problems are solvable because the underlying data has a much simpler, low-dimensional structure. Regularization is the tool used to enforce these structural priors (sparsity, low-rank).
* **Convex Relaxation is Key:** Many of these problems are naturally formulated with non-convex functions like the L0 norm or rank, which are computationally intractable. The standard strategy is to relax them into their closest convex-and-tractable approximations: the **L1 norm for sparsity** and the **nuclear norm for low-rankness**.
* **Decomposition for Analysis:** Robust PCA and Sparse-Smooth Decomposition show the power of modeling observed data as a sum of distinct components (e.g., low-rank + sparse, or smooth + sparse). This allows for separating signal from noise, background from foreground, or normal behavior from anomalies.
* **The Kernel Trick for Non-Linearity:** Kernel methods provide a powerful and elegant way to extend linear models like ridge regression to handle highly non-linear data, with the smoothness of the result controlled by the regularization term.


## Conclusion

### Books

- Elements of Statistical Learning
- An Introduction to Statistical Learning
- Image Book1
- Image Book2
- Tensor Data
- Convex Optimization