---
title: 'Reflections on ICML 2025'
description: 'Some notes and observations from my time at the 2025 International Conference for Machine Learning (ICML)'
date: '2025-07-30'
status: 'complete'
type: 'retro'
tags: ['machine-learning', 'deep-learning', 'short']
category: ['retros']
draft: false
audience: 'All'
media_subpath: "/ideas/2025/icml/"
image:
  path: "./icml-25-vancouver.png"
  alt: 'ICML 2025 in Vancouver'
hideCaption: true
---

A few weeks out from ICML, I've been reflecting on what stuck with me — not just the research, but the experience of being there in the middle of it all. This was my first academic conference and it was both invigorating and humbling.

## By the numbers

ICML 2025 was huge:
	- 9,787 attendees (~90% in person)
	- 3,333 accepted papers out of 12,468 submitted -- supported by over 11,000 reviewers
	- 33 workshops, 11 tutorials, and 29 expo events

Motivated by intellectual curiosity, I spent my time trying to  soak up as much information as I could. Hours of consecutive, 12-minute presentations flew by.

It's easy to feel small in a sea of nearly 10,000 people, but surprisingly, many attendees were solo or in pairs. That made it easier to start conversations, especially in poster sessions where enthusiasm and curiosity go a long way. Walking the halls of posters drove home how broad and fragmented the field is. Machine learning isn't one discipline — it's hundreds of niches moving in parallel.

## A few takeaways:
	- It's okay to ask "basic" questions. People generally like talking about their work.
	- Specialization is the norm. Most researchers go deep on one or two topics — not everything.
	- Momentum helps. Just walk up and say hi. Connect and exit.

## Research highlights

The top research themes this year? Not a surprise:
	- LLMs dominated, with the most papers (438) in the bucket of "deep learning: large language models." 
	- Followed by computer vision (178), generative modeling (167), applications in chemistry and earth sciences (115), and graph neural nets (105).

For me, some of the most interesting work centered around masked language models and diffusion:
	- [Train for the Worst, Plan for the Best: Understanding Token Ordering in Masked Diffusions ](https://arxiv.org/abs/2502.06768)
	- [ Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction ](https://arxiv.org/abs/2504.15266)

Notes to future me
	- Be more meticulous about what you saw and why it was interesting.
	- Capture insights during the conference — not just after.
	- Networking matters. Cold outreach often works better than expected.
  - It's okay to reach out to researchers, this is how jobs and PhD programs are found.
	- Failing is part of the process. So is not understanding everything.

Already looking forward to the next conference!
